"""
åŸºäºElasticsearchçš„å‘é‡æ£€ç´¢æœåŠ¡

ç”¨äºæ•°æ®é›†çš„å‘é‡å­˜å‚¨å’Œè¯­ä¹‰æœç´¢
"""

import json
from typing import List, Dict, Any, Optional
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
import numpy as np
from ..utils.logger import get_logger

class VectorSearchService:
    """ESå‘é‡æ£€ç´¢æœåŠ¡"""
    
    def __init__(self, es_config: Dict[str, Any], index_name: str = "intent_recognition"):
        """
        åˆå§‹åŒ–ESå‘é‡æ£€ç´¢æœåŠ¡
        
        Args:
            es_config: ESé…ç½®å­—å…¸
                {
                    'hosts': ['localhost:9200'],
                    'username': 'elastic',
                    'password': 'password',
                    'verify_certs': False
                }
            index_name: ESç´¢å¼•åç§°
        """
        self.logger = get_logger(__name__)
        self.es_config = es_config
        self.index_name = index_name
        
        try:
            # åˆå§‹åŒ–ESå®¢æˆ·ç«¯
            self.es = Elasticsearch(**es_config)
            self.logger.info(f"ESè¿æ¥æˆåŠŸ: {es_config.get('hosts', ['localhost:9200'])}")
            
            # ç¡®ä¿ç´¢å¼•å­˜åœ¨
            self._ensure_index_exists()
            
        except Exception as e:
            self.logger.error(f"ESè¿æ¥å¤±è´¥: {e}")
            raise
    
    def _ensure_index_exists(self):
        """ç¡®ä¿å‘é‡ç´¢å¼•å­˜åœ¨"""
        try:
            # é¦–å…ˆå°è¯•è·å–ESé›†ç¾¤ä¿¡æ¯ä»¥éªŒè¯è¿æ¥
            info = self.es.info()
            self.logger.info(f"ESé›†ç¾¤ä¿¡æ¯: {info['version']['number']}")
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            exists = self.es.indices.exists(index=self.index_name)
            self.logger.info(f"ç´¢å¼• {self.index_name} å­˜åœ¨çŠ¶æ€: {exists}")
            
            if not exists:
                self.logger.info(f"åˆ›å»ºESç´¢å¼•: {self.index_name}")
                
                # ç´¢å¼•æ˜ å°„é…ç½® - ç®€åŒ–ç‰ˆæœ¬ä»¥æé«˜å…¼å®¹æ€§
                mapping = {
                    "mappings": {
                        "properties": {
                            "dataset_id": {"type": "keyword"},
                            "name": {"type": "text"},
                            "description": {"type": "text"},
                            "keywords": {"type": "keyword"},
                            "domain": {"type": "keyword"},
                            "data_summary": {"type": "text"},
                            "columns_info": {"type": "text"},
                            "tree_node_id": {"type": "keyword"},
                            "file_path": {"type": "keyword"},
                            "status": {"type": "keyword"},
                            "embedding": {
                                "type": "dense_vector",
                                "dims": 1024
                            },
                            "created_at": {"type": "date"},
                            "updated_at": {"type": "date"}
                        }
                    },
                    "settings": {
                        "number_of_shards": 1,
                        "number_of_replicas": 0
                    }
                }
                
                try:
                    # ä½¿ç”¨å…¼å®¹æ€§æ›´å¥½çš„APIè°ƒç”¨æ–¹å¼
                    self.es.indices.create(index=self.index_name, **mapping)
                    self.logger.info("ESç´¢å¼•åˆ›å»ºæˆåŠŸ")
                except Exception as e:
                    self.logger.error(f"ESç´¢å¼•åˆ›å»ºå¤±è´¥: {e}")
                    # å°è¯•ä½¿ç”¨æ›´ç®€å•çš„ç´¢å¼•ç»“æ„
                    self.logger.info("å°è¯•åˆ›å»ºç®€åŒ–ç´¢å¼•ç»“æ„")
                    simple_mapping = {
                        "mappings": {
                            "properties": {
                                "dataset_id": {"type": "keyword"},
                                "name": {"type": "text"},
                                "description": {"type": "text"},
                                "embedding": {
                                    "type": "dense_vector",
                                    "dims": 1024
                                }
                            }
                        }
                    }
                    self.es.indices.create(index=self.index_name, **simple_mapping)
                    self.logger.info("ç®€åŒ–ESç´¢å¼•åˆ›å»ºæˆåŠŸ")
        
        except Exception as e:
            self.logger.error(f"ESç´¢å¼•æ£€æŸ¥/åˆ›å»ºå¤±è´¥: {e}")
            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {type(e).__name__}: {str(e)}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸æœåŠ¡ç»§ç»­è¿è¡Œä½†è®°å½•é”™è¯¯
            self.logger.warning("ESç´¢å¼•åˆå§‹åŒ–å¤±è´¥ï¼Œå‘é‡æœç´¢åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
    
    def vector_search(self, query_embedding: np.ndarray, size: int = 10, 
                     min_score: float = 0.5, filters: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """
        åŸºäºå‘é‡çš„è¯­ä¹‰æœç´¢
        
        Args:
            query_embedding: æŸ¥è¯¢å‘é‡
            size: è¿”å›ç»“æœæ•°é‡
            min_score: æœ€å°ç›¸ä¼¼åº¦åˆ†æ•°
            filters: é¢å¤–è¿‡æ»¤æ¡ä»¶
            
        Returns:
            List[Dict]: æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # æ£€æŸ¥æŸ¥è¯¢å‘é‡æ˜¯å¦æœ‰æ•ˆ
            if isinstance(query_embedding, np.ndarray):
                if np.any(np.isnan(query_embedding)) or np.any(np.isinf(query_embedding)):
                    self.logger.warning("æŸ¥è¯¢å‘é‡åŒ…å«æ— æ•ˆå€¼(NaNæˆ–Inf)ï¼Œè¿”å›ç©ºç»“æœ")
                    return []
                query_vector = query_embedding.tolist()
            else:
                query_vector = query_embedding
            
            # æ„å»ºæŸ¥è¯¢ï¼Œæ·»åŠ NaNæ£€æŸ¥
            query = {
                "script_score": {
                    "query": {"match_all": {}},
                    "script": {
                        "source": """
                            double similarity = cosineSimilarity(params.query_vector, 'embedding');
                            if (Double.isNaN(similarity) || Double.isInfinite(similarity)) {
                                return 0.0;
                            }
                            return similarity + 1.0;
                        """,
                        "params": {
                            "query_vector": query_vector
                        }
                    }
                }
            }
            
            # æ·»åŠ è¿‡æ»¤æ¡ä»¶
            if filters:
                filter_clauses = []
                for key, value in filters.items():
                    if key == "status" and value:
                        filter_clauses.append({"term": {"status": value}})
                    elif key == "domain" and value:
                        filter_clauses.append({"term": {"domain": value}})
                    elif key == "tree_node_id" and value:
                        filter_clauses.append({"term": {"tree_node_id": value}})
                
                if filter_clauses:
                    query["script_score"]["query"] = {
                        "bool": {
                            "filter": filter_clauses
                        }
                    }
            
            # æ‰§è¡Œæœç´¢
            response = self.es.search(
                index=self.index_name,
                query=query,
                size=size,
                source_excludes=["embedding"]
            )
            
            results = []
            for hit in response['hits']['hits']:
                # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆESè¿”å›çš„åˆ†æ•°éœ€è¦å‡1ï¼‰
                similarity_score = hit['_score'] - 1.0
                
                if similarity_score >= min_score:
                    result = hit['_source']
                    result['similarity_score'] = similarity_score
                    results.append(result)
            
            self.logger.debug(f"å‘é‡æœç´¢å®Œæˆ: æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ")
            return results
            
        except Exception as e:
            self.logger.error(f"å‘é‡æœç´¢å¤±è´¥: {e}")
            return []
    
    def index_dataset(self, dataset_id: str, dataset_info: Dict[str, Any], embedding: np.ndarray) -> bool:
        """
        ç´¢å¼•å•ä¸ªæ•°æ®é›†
        
        Args:
            dataset_id: æ•°æ®é›†ID
            dataset_info: æ•°æ®é›†ä¿¡æ¯
            embedding: æ•°æ®é›†å‘é‡
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            # æ£€æŸ¥åµŒå…¥å‘é‡æ˜¯å¦æœ‰æ•ˆ
            if isinstance(embedding, np.ndarray):
                if np.any(np.isnan(embedding)) or np.any(np.isinf(embedding)):
                    self.logger.warning(f"æ•°æ®é›† {dataset_id} çš„åµŒå…¥å‘é‡åŒ…å«æ— æ•ˆå€¼ï¼Œè·³è¿‡ç´¢å¼•")
                    return False
                embedding_list = embedding.tolist()
            else:
                embedding_list = embedding
            
            self.logger.debug(f"ğŸ“ å¼€å§‹ç´¢å¼•æ•°æ®é›† {dataset_id}: {dataset_info.get('name', 'Unknown')}")
            
            # éªŒè¯embedding
            if embedding is None:
                self.logger.error(f"âŒ æ•°æ®é›† {dataset_id} çš„embeddingä¸ºNone")
                return False
            
            self.logger.debug(f"ğŸ“Š Embeddingç»´åº¦: {embedding.shape if isinstance(embedding, np.ndarray) else len(embedding_list) if hasattr(embedding_list, '__len__') else 'unknown'}")
            
            doc = {
                "dataset_id": dataset_id,
                "name": dataset_info.get("name", ""),
                "description": dataset_info.get("description", ""),
                "keywords": dataset_info.get("keywords", []),
                "domain": dataset_info.get("domain", ""),
                "data_summary": dataset_info.get("data_summary", ""),
                "columns_info": dataset_info.get("columns_info", ""),
                "tree_node_id": dataset_info.get("tree_node_id", ""),
                "file_path": dataset_info.get("file_path", ""),
                "status": dataset_info.get("status", "active"),
                "embedding": embedding_list,
                "created_at": dataset_info.get("created_at"),
                "updated_at": dataset_info.get("updated_at")
            }
            
            self.logger.debug(f"ğŸ“‹ æ„å»ºæ–‡æ¡£å®Œæˆï¼Œå­—æ®µæ•°: {len(doc)}")
            
            # æ£€æŸ¥ESè¿æ¥çŠ¶æ€
            try:
                es_info = self.es.info()
                self.logger.debug(f"ğŸ”— ESè¿æ¥æ­£å¸¸: {es_info['version']['number']}")
            except Exception as conn_e:
                self.logger.error(f"âŒ ESè¿æ¥å¤±è´¥: {conn_e}")
                return False
            
            # æ‰§è¡Œç´¢å¼•æ“ä½œ
            self.logger.debug(f"ğŸ“ æ‰§è¡ŒESç´¢å¼•æ“ä½œ...")
            response = self.es.index(
                index=self.index_name,
                id=dataset_id,
                document=doc
            )
            
            self.logger.debug(f"âœ… æ•°æ®é›†ç´¢å¼•æˆåŠŸ: {dataset_id}, ESå“åº”: {response.get('result', 'unknown')}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®é›†ç´¢å¼•å¤±è´¥ {dataset_id}: {e}")
            self.logger.error(f"âŒ ç´¢å¼•é”™è¯¯è¯¦æƒ…: {type(e).__name__}: {str(e)}")
            
            # ç‰¹æ®Šé”™è¯¯å¤„ç†
            if "ConnectionError" in str(type(e)):
                self.logger.error("âŒ ESè¿æ¥é”™è¯¯ï¼Œè¯·æ£€æŸ¥ElasticsearchæœåŠ¡çŠ¶æ€")
            elif "RequestError" in str(type(e)):
                self.logger.error("âŒ ESè¯·æ±‚é”™è¯¯ï¼Œå¯èƒ½æ˜¯ç´¢å¼•æ˜ å°„æˆ–æ•°æ®æ ¼å¼é—®é¢˜")
            elif "timeout" in str(e).lower():
                self.logger.error("âŒ ESæ“ä½œè¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒESæ€§èƒ½")
            
            import traceback
            self.logger.error(f"âŒ ç´¢å¼•é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return False
    
    def bulk_index_datasets(self, datasets: List[Dict[str, Any]]) -> int:
        """
        æ‰¹é‡ç´¢å¼•æ•°æ®é›†
        
        Args:
            datasets: æ•°æ®é›†åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«dataset_id, dataset_info, embedding
            
        Returns:
            int: æˆåŠŸç´¢å¼•çš„æ•°é‡
        """
        if not datasets:
            return 0
        
        try:
            actions = []
            for item in datasets:
                dataset_id = item["dataset_id"]
                dataset_info = item["dataset_info"]
                embedding = item["embedding"]
                
                doc = {
                    "_index": self.index_name,
                    "_id": dataset_id,
                    "_source": {
                        "dataset_id": dataset_id,
                        "name": dataset_info.get("name", ""),
                        "description": dataset_info.get("description", ""),
                        "keywords": dataset_info.get("keywords", []),
                        "domain": dataset_info.get("domain", ""),
                        "data_summary": dataset_info.get("data_summary", ""),
                        "columns_info": dataset_info.get("columns_info", ""),
                        "tree_node_id": dataset_info.get("tree_node_id", ""),
                        "file_path": dataset_info.get("file_path", ""),
                        "status": dataset_info.get("status", "active"),
                        "embedding": embedding.tolist() if isinstance(embedding, np.ndarray) else embedding,
                        "created_at": dataset_info.get("created_at"),
                        "updated_at": dataset_info.get("updated_at")
                    }
                }
                actions.append(doc)
            
            # æ‰§è¡Œæ‰¹é‡ç´¢å¼•
            success_count, failed_items = bulk(self.es, actions)
            
            if failed_items:
                self.logger.warning(f"æ‰¹é‡ç´¢å¼•éƒ¨åˆ†å¤±è´¥: {len(failed_items)} ä¸ªå¤±è´¥")
                for item in failed_items:
                    self.logger.error(f"å¤±è´¥é¡¹: {item}")
            
            self.logger.info(f"æ‰¹é‡ç´¢å¼•å®Œæˆ: {success_count} ä¸ªæˆåŠŸ")
            return success_count
            
        except Exception as e:
            self.logger.error(f"æ‰¹é‡ç´¢å¼•å¤±è´¥: {e}")
            return 0
    
    def hybrid_search(self, query_text: str, query_embedding: np.ndarray, 
                     size: int = 10, text_weight: float = 0.3, vector_weight: float = 0.7,
                     filters: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """
        æ··åˆæœç´¢ï¼šç»“åˆæ–‡æœ¬æœç´¢å’Œå‘é‡æœç´¢
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            query_embedding: æŸ¥è¯¢å‘é‡
            size: è¿”å›ç»“æœæ•°é‡
            text_weight: æ–‡æœ¬æœç´¢æƒé‡
            vector_weight: å‘é‡æœç´¢æƒé‡
            filters: è¿‡æ»¤æ¡ä»¶
            
        Returns:
            List[Dict]: æœç´¢ç»“æœ
        """
        try:
            # æ„å»ºæ··åˆæŸ¥è¯¢
            query = {
                "bool": {
                    "should": [
                        # æ–‡æœ¬æœç´¢éƒ¨åˆ†
                        {
                            "multi_match": {
                                "query": query_text,
                                "fields": ["name^2", "description", "keywords", "data_summary"],
                                "type": "best_fields",
                                "boost": text_weight
                            }
                        },
                        # å‘é‡æœç´¢éƒ¨åˆ†
                        {
                            "script_score": {
                                "query": {"match_all": {}},
                                "script": {
                                    "source": f"(cosineSimilarity(params.query_vector, 'embedding') + 1.0) * {vector_weight}",
                                    "params": {
                                        "query_vector": query_embedding.tolist() if isinstance(query_embedding, np.ndarray) else query_embedding
                                    }
                                }
                            }
                        }
                    ]
                }
            }
            
            # æ·»åŠ è¿‡æ»¤æ¡ä»¶
            if filters:
                filter_clauses = []
                for key, value in filters.items():
                    if key == "status" and value:
                        filter_clauses.append({"term": {"status": value}})
                    elif key == "domain" and value:
                        filter_clauses.append({"term": {"domain": value}})
                    elif key == "tree_node_id" and value:
                        filter_clauses.append({"term": {"tree_node_id": value}})
                
                if filter_clauses:
                    query["bool"]["filter"] = filter_clauses
            
            # æ‰§è¡Œæœç´¢
            response = self.es.search(
                index=self.index_name,
                query=query,
                size=size,
                source_excludes=["embedding"]
            )
            
            results = []
            for hit in response['hits']['hits']:
                result = hit['_source']
                result['hybrid_score'] = hit['_score']
                results.append(result)
            
            self.logger.debug(f"æ··åˆæœç´¢å®Œæˆ: æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            self.logger.error(f"æ··åˆæœç´¢å¤±è´¥: {e}")
            return []
    
    def delete_dataset(self, dataset_id: str) -> bool:
        """åˆ é™¤æ•°æ®é›†ç´¢å¼•"""
        try:
            response = self.es.delete(index=self.index_name, id=dataset_id)
            self.logger.debug(f"æ•°æ®é›†ç´¢å¼•åˆ é™¤æˆåŠŸ: {dataset_id}")
            return True
        except Exception as e:
            self.logger.error(f"æ•°æ®é›†ç´¢å¼•åˆ é™¤å¤±è´¥ {dataset_id}: {e}")
            return False
    
    def update_dataset(self, dataset_id: str, dataset_info: Dict[str, Any], embedding: np.ndarray = None) -> bool:
        """æ›´æ–°æ•°æ®é›†ç´¢å¼•"""
        try:
            update_body = {
                "doc": {
                    "name": dataset_info.get("name", ""),
                    "description": dataset_info.get("description", ""),
                    "keywords": dataset_info.get("keywords", []),
                    "domain": dataset_info.get("domain", ""),
                    "data_summary": dataset_info.get("data_summary", ""),
                    "columns_info": dataset_info.get("columns_info", ""),
                    "status": dataset_info.get("status", "active"),
                    "updated_at": dataset_info.get("updated_at")
                }
            }
            
            if embedding is not None:
                update_body["doc"]["embedding"] = embedding.tolist() if isinstance(embedding, np.ndarray) else embedding
            
            response = self.es.update(
                index=self.index_name,
                id=dataset_id,
                doc=update_body["doc"]
            )
            
            self.logger.debug(f"æ•°æ®é›†ç´¢å¼•æ›´æ–°æˆåŠŸ: {dataset_id}")
            return True
            
        except Exception as e:
            self.logger.error(f"æ•°æ®é›†ç´¢å¼•æ›´æ–°å¤±è´¥ {dataset_id}: {e}")
            return False
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = self.es.indices.stats(index=self.index_name)
            count_response = self.es.count(index=self.index_name)
            
            return {
                "index_name": self.index_name,
                "document_count": count_response["count"],
                "index_size": stats["indices"][self.index_name]["total"]["store"]["size_in_bytes"],
                "status": "healthy"
            }
        except Exception as e:
            self.logger.error(f"è·å–ç´¢å¼•ç»Ÿè®¡å¤±è´¥: {e}")
            return {
                "index_name": self.index_name,
                "status": "error",
                "error": str(e)
            } 