"""
æ•°æ®åˆ†æå™¨æ¨¡å—

æä¾›æ•°æ®ç»“æ„åˆ†æå’Œé¢„å¤„ç†åŠŸèƒ½ï¼Œæ”¯æŒPandasAI v3è¯­ä¹‰å±‚
"""

import pandas as pd
import pandasai as pai
import json
import os
import re
import time
from typing import Dict, List, Any, Optional
from ..utils.logger import get_logger, LogContext
from ..utils.schema_database import SchemaDatabase
from ..utils.transformations_helper import TransformationsHelper
from ..config.settings import Settings

# å¯¼å…¥å­—ä½“é…ç½®ï¼Œè§£å†³matplotlibä¸­æ–‡å­—ä½“é—®é¢˜
try:
    from ..utils.font_config import auto_configure
    auto_configure()  # è‡ªåŠ¨é…ç½®å­—ä½“
except ImportError as e:
    get_logger(__name__).warning(f"å­—ä½“é…ç½®æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

class DataAnalyzer:
    """æ•°æ®ç»“æ„åˆ†æå™¨"""
    
    @staticmethod
    def analyze_structure(df):
        """åˆ†ææ•°æ®ç»“æ„ï¼Œä¸ºAIæä¾›æ›´å¥½çš„ä¸Šä¸‹æ–‡"""
        logger = get_logger(__name__)
        
        # è·å–åŸå§‹pandas DataFrame
        raw_df = df._df if hasattr(df, '_df') else df
        logger.debug(f"åˆ†ææ•°æ®ç»“æ„ - å½¢çŠ¶: {raw_df.shape}")
        
        # åŸºæœ¬ä¿¡æ¯  
        info = {
            "shape": raw_df.shape,
            "columns": list(raw_df.columns),
            "dtypes": {col: str(dtype) for col, dtype in raw_df.dtypes.to_dict().items()}
        }
        
        # åˆ†ææ—¥æœŸåˆ—  
        date_columns = []
        for col in raw_df.columns:
            # æ£€æŸ¥æ˜¯å¦ä¸ºdatetimeç±»å‹ã€æ˜ç¡®çš„æ—¥æœŸåˆ—åæˆ–ç‰¹å®šçš„å¹´ä»½å­—æ®µ
            if (pd.api.types.is_datetime64_any_dtype(raw_df[col]) or 
                'date' in col.lower() or 
                col == 'SJNF'):  # ç‰¹åˆ«è¯†åˆ«SJNFï¼ˆæ•°æ®å¹´ä»½ï¼‰å­—æ®µ
                date_columns.append(col)
                # è·å–æ—¥æœŸèŒƒå›´
                info[f"{col}_range"] = f"{raw_df[col].min()} åˆ° {raw_df[col].max()}"
        
        # åˆ†æåˆ†ç±»åˆ—
        categorical_info = {}
        for col in raw_df.columns:
            if raw_df[col].dtype == 'object' and col not in date_columns:
                unique_vals = raw_df[col].unique()
                if len(unique_vals) <= 20:  # å¦‚æœå”¯ä¸€å€¼ä¸å¤šï¼Œå±•ç¤ºæ‰€æœ‰å€¼
                    categorical_info[col] = list(unique_vals)
                else:
                    categorical_info[col] = f"å…±{len(unique_vals)}ä¸ªä¸åŒå€¼"
        
        info["date_columns"] = date_columns
        info["categorical_info"] = categorical_info
        
        logger.debug(f"åˆ†æå®Œæˆ - æ—¥æœŸåˆ—: {len(date_columns)}, åˆ†ç±»åˆ—: {len(categorical_info)}")
        return info
    
    @staticmethod
    def load_data(file_path, **kwargs):
        """æ™ºèƒ½åŠ è½½æ•°æ®ï¼Œè‡ªåŠ¨å¤„ç†å¸¸è§æ ¼å¼é—®é¢˜"""
        logger = get_logger(__name__)
        logger.info(f"åŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")
        
        try:
            # å°è¯•è‡ªåŠ¨æ£€æµ‹æ—¥æœŸåˆ—å¹¶ä¼˜åŒ–è§£æ
            if file_path.endswith('.csv'):
                # å…ˆè¯»å–å‡ è¡Œæ¥æ£€æµ‹æ—¥æœŸåˆ—
                sample_df = pd.read_csv(file_path, nrows=5)
                
                # æ£€æµ‹å¯èƒ½çš„æ—¥æœŸåˆ—
                date_columns = []
                for col in sample_df.columns:
                    if 'date' in col.lower():
                        date_columns.append(col)
                
                # é‡æ–°è¯»å–ï¼Œä¼˜åŒ–æ—¥æœŸè§£æ
                if date_columns:
                    logger.debug(f"æ£€æµ‹åˆ°æ—¥æœŸåˆ—: {date_columns}")
                    df = pd.read_csv(file_path, 
                                   parse_dates=date_columns, 
                                   date_format='%Y-%m-%d',
                                   **kwargs)
                else:
                    df = pd.read_csv(file_path, **kwargs)
                
                logger.info(f"æ•°æ®åŠ è½½æˆåŠŸ - å½¢çŠ¶: {df.shape}")
                return df
            else:
                # å…¶ä»–æ ¼å¼çš„æ–‡ä»¶
                if file_path.endswith('.xlsx'):
                    return pd.read_excel(file_path, **kwargs)
                elif file_path.endswith('.json'):
                    return pd.read_json(file_path, **kwargs)
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
                    
        except Exception as e:
            raise ValueError(f"æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
    
    @staticmethod
    def create_semantic_schema(file_path: str, organization: str = "ask-data-ai", 
                              dataset_name: str = "default") -> Dict[str, Any]:
        """
        ä¸ºæ•°æ®æ–‡ä»¶åˆ›å»ºè¯­ä¹‰å±‚é…ç½®
        """
        # æ ‡å‡†åŒ–ç»„ç»‡åç§°ï¼šè½¬å°å†™ï¼Œæ›¿æ¢ç‰¹æ®Šå­—ç¬¦ä¸ºè¿å­—ç¬¦
        def standardize_name(name: str) -> str:
            # ç§»é™¤ä¸­æ–‡å­—ç¬¦å’Œç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯æ•°å­—
            name = re.sub(r'[^\w\s-]', '', name)
            # æ›¿æ¢ç©ºæ ¼å’Œä¸‹åˆ’çº¿ä¸ºè¿å­—ç¬¦
            name = re.sub(r'[\s_]+', '-', name)
            # è½¬å°å†™
            name = name.lower()
            # ç§»é™¤å¤šä½™çš„è¿å­—ç¬¦
            name = re.sub(r'-+', '-', name)
            # ç§»é™¤é¦–å°¾è¿å­—ç¬¦
            name = name.strip('-')
            # å¦‚æœä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å€¼
            return name if name else 'default'
        
        # æ ‡å‡†åŒ–åç§°
        organization = standardize_name(organization)
        dataset_name = standardize_name(dataset_name)
        
        # å…ˆåˆ†ææ•°æ®ç»“æ„
        df = pd.read_csv(file_path)
        data_info = DataAnalyzer.analyze_structure(df)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºåœŸåœ°æ•°æ®ï¼ˆåŒ…å«å…³é”®å­—æ®µï¼‰
        is_land_data = any(field in data_info['columns'] for field in ['ZLDWDM', 'DLBM', 'TBMJ', 'QSXZ'])
        
        if is_land_data:
            # ä½¿ç”¨æ ‡å‡†åœŸåœ°æ•°æ®å­—æ®µé…ç½®
            columns = DataAnalyzer._generate_standard_land_columns(data_info['columns'])
        else:
            # ä½¿ç”¨åŸæœ‰çš„é€šç”¨é…ç½®é€»è¾‘
            columns = []
            for col in data_info['columns']:
                dtype = data_info['dtypes'][col]
                
                # æ˜ å°„pandasæ•°æ®ç±»å‹åˆ°è¯­ä¹‰å±‚ç±»å‹
                if pd.api.types.is_datetime64_any_dtype(dtype):
                    semantic_type = "datetime"
                    description = f"æ—¥æœŸæ—¶é—´åˆ— - {col}"
                elif pd.api.types.is_integer_dtype(dtype):
                    semantic_type = "integer"
                    description = f"æ•´æ•°åˆ— - {col}ï¼Œè¡¨ç¤ºæ•°é‡æˆ–è®¡æ•°"
                elif pd.api.types.is_float_dtype(dtype):
                    semantic_type = "float"
                    description = f"æµ®ç‚¹æ•°åˆ— - {col}ï¼Œè¡¨ç¤ºé‡‘é¢æˆ–æ¯”ä¾‹"
                elif pd.api.types.is_bool_dtype(dtype):
                    semantic_type = "boolean"
                    description = f"å¸ƒå°”åˆ— - {col}ï¼Œè¡¨ç¤ºæ˜¯/å¦çŠ¶æ€"
                else:
                    semantic_type = "string"
                    if col in data_info['categorical_info']:
                        categories = data_info['categorical_info'][col]
                        if isinstance(categories, list):
                            description = f"åˆ†ç±»åˆ— - {col}ï¼Œå¯èƒ½å€¼: {', '.join(map(str, categories[:5]))}"
                        else:
                            description = f"åˆ†ç±»åˆ— - {col}ï¼Œ{categories}"
                    else:
                        description = f"æ–‡æœ¬åˆ— - {col}"
                
                columns.append({
                    "name": col,
                    "type": semantic_type,
                    "description": description
                })
        
        # æ„å»ºè¯­ä¹‰å±‚é…ç½®
        schema = {
            "path": f"{organization}/{dataset_name}",
            "description": f"æ¥è‡ª{os.path.basename(file_path)}çš„æ•°æ®é›†ï¼ŒåŒ…å«{data_info['shape'][0]}è¡Œè®°å½•",
            "columns": columns
        }
        
        # å¦‚æœæœ‰æ—¥æœŸåˆ—ï¼Œæ·»åŠ åˆ†ç»„é…ç½®
        if data_info['date_columns']:
            schema["group_by"] = data_info['date_columns']
        
        return schema

    @staticmethod
    def _generate_standard_land_columns(actual_columns: List[str]) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆæ ‡å‡†åœŸåœ°æ•°æ®å­—æ®µé…ç½®
        
        Args:
            actual_columns: å®é™…æ•°æ®ä¸­çš„å­—æ®µåˆ—è¡¨
            
        Returns:
            æ ‡å‡†åŒ–çš„å­—æ®µé…ç½®åˆ—è¡¨
        """
        # æ ‡å‡†åœŸåœ°æ•°æ®å­—æ®µé…ç½®æ˜ å°„è¡¨
        standard_fields = {
            'BSM': {
                'type': 'string',
                'description': 'æ ‡è¯†ç  - å›¾æ–‘çš„å”¯ä¸€æ ‡è¯†ç¬¦'
            },
            'YSDM': {
                'type': 'string', 
                'description': 'è¦ç´ ä»£ç  - åœ°ç†è¦ç´ åˆ†ç±»ä»£ç '
            },
            'TBYBH': {
                'type': 'string',
                'description': 'å›¾æ–‘é¢„ç¼–å· - å›¾æ–‘çš„é¢„åˆ†é…ç¼–å·'
            },
            'TBBH': {
                'type': 'string',
                'description': 'å›¾æ–‘ç¼–å· - å›¾æ–‘çš„æ­£å¼ç¼–å·'
            },
            'DLBM': {
                'type': 'string',
                'description': 'åœ°ç±»ç¼–ç  - åœŸåœ°åˆ©ç”¨ç±»å‹ç¼–ç '
            },
            'DLMC': {
                'type': 'string',
                'description': 'åœ°ç±»åç§° - åœŸåœ°åˆ©ç”¨ç±»å‹åç§°'
            },
            'QSXZ': {
                'type': 'integer',
                'description': 'æƒå±æ€§è´¨ - åœŸåœ°æƒå±æ€§è´¨ä»£ç '
            },
            'QSDWDM': {
                'type': 'integer',
                'description': 'æƒå±å•ä½ä»£ç  - åœŸåœ°æƒå±å•ä½ç¼–ç '
            },
            'QSDWMC': {
                'type': 'string',
                'description': 'æƒå±å•ä½åç§° - åœŸåœ°æƒå±å•ä½åç§°'
            },
            'ZLDWDM': {
                'type': 'integer',
                'description': 'åè½å•ä½ä»£ç  - å›¾æ–‘æ‰€åœ¨è¡Œæ”¿åŒºåˆ’ä»£ç '
            },
            'ZLDWMC': {
                'type': 'string',
                'description': 'åè½å•ä½åç§° - å›¾æ–‘æ‰€åœ¨è¡Œæ”¿åŒºåˆ’åç§°'
            },
            'TBMJ': {
                'type': 'float',
                'description': 'å›¾æ–‘é¢ç§¯ - å›¾æ–‘çš„å®é™…é¢ç§¯ï¼ˆå¹³æ–¹ç±³ï¼‰'
            },
            'KCDLBM': {
                'type': 'string',
                'description': 'æ‰£é™¤åœ°ç±»ç¼–ç  - éœ€è¦æ‰£é™¤éƒ¨åˆ†çš„åœ°ç±»ç¼–ç '
            },
            'KCXS': {
                'type': 'float',
                'description': 'æ‰£é™¤åœ°ç±»ç³»æ•° - æ‰£é™¤é¢ç§¯çš„è®¡ç®—ç³»æ•°'
            },
            'KCMJ': {
                'type': 'float',
                'description': 'æ‰£é™¤åœ°ç±»é¢ç§¯ - éœ€è¦æ‰£é™¤çš„é¢ç§¯ï¼ˆå¹³æ–¹ç±³ï¼‰'
            },
            'TBDLMJ': {
                'type': 'float',
                'description': 'å›¾æ–‘åœ°ç±»é¢ç§¯ - æ‰£é™¤åçš„å®é™…åœ°ç±»é¢ç§¯ï¼ˆå¹³æ–¹ç±³ï¼‰'
            },
            'GDLX': {
                'type': 'string',
                'description': 'è€•åœ°ç±»å‹ - è€•åœ°çš„å…·ä½“ç±»å‹åˆ†ç±»'
            },
            'GDPDJB': {
                'type': 'string',
                'description': 'è€•åœ°å¡åº¦çº§åˆ« - è€•åœ°çš„å¡åº¦ç­‰çº§'
            },
            'XZDWKD': {
                'type': 'float',
                'description': 'çº¿çŠ¶åœ°ç‰©å®½åº¦ - çº¿æ€§åœ°ç‰©çš„å®½åº¦ï¼ˆç±³ï¼‰'
            },
            'TBXHDM': {
                'type': 'string',
                'description': 'å›¾æ–‘ç»†åŒ–ä»£ç  - å›¾æ–‘çš„ç»†åŒ–åˆ†ç±»ä»£ç '
            },
            'TBXHMC': {
                'type': 'string',
                'description': 'å›¾æ–‘ç»†åŒ–åç§° - å›¾æ–‘çš„ç»†åŒ–åˆ†ç±»åç§°'
            },
            'ZZSXDM': {
                'type': 'string',
                'description': 'ç§æ¤å±æ€§ä»£ç  - è€•åœ°ç§æ¤å±æ€§åˆ†ç±»ä»£ç '
            },
            'ZZSXMC': {
                'type': 'string',
                'description': 'ç§æ¤å±æ€§åç§° - è€•åœ°ç§æ¤å±æ€§åˆ†ç±»åç§°'
            },
            'GDDB': {
                'type': 'integer',
                'description': 'è€•åœ°ç­‰åˆ« - è€•åœ°è´¨é‡ç­‰çº§'
            },
            'FRDBS': {
                'type': 'string',
                'description': 'é£å…¥åœ°æ ‡è¯† - æ˜¯å¦ä¸ºé£å…¥åœ°çš„æ ‡è¯†'
            },
            'CZCSXM': {
                'type': 'string',
                'description': 'åŸé•‡æ‘å±æ€§ç  - åŸé•‡æ‘ç”¨åœ°å±æ€§ä»£ç '
            },
            'SJNF': {
                'type': 'integer',
                'description': 'æ•°æ®å¹´ä»½ - æ•°æ®è°ƒæŸ¥çš„å¹´ä»½'
            },
            'MSSM': {
                'type': 'string',
                'description': 'æè¿°è¯´æ˜ - å›¾æ–‘çš„è¯¦ç»†æè¿°ä¿¡æ¯'
            },
            'HDMC': {
                'type': 'string',
                'description': 'æµ·å²›åç§° - æµ·å²›åœ°åŒºçš„åç§°'
            },
            'BZ': {
                'type': 'string',
                'description': 'å¤‡æ³¨ - å…¶ä»–å¤‡æ³¨ä¿¡æ¯'
            },
            # å‡ ä½•å±æ€§å­—æ®µ
            'Shape_Leng': {
                'type': 'float',
                'description': 'å½¢çŠ¶é•¿åº¦ - å›¾æ–‘è¾¹ç•Œçš„å‘¨é•¿ï¼ˆç±³ï¼‰'
            },
            'Shape_Area': {
                'type': 'float',
                'description': 'å½¢çŠ¶é¢ç§¯ - å›¾æ–‘çš„å‡ ä½•é¢ç§¯ï¼ˆå¹³æ–¹ç±³ï¼‰'
            },
            'centroid_x': {
                'type': 'float',
                'description': 'è´¨å¿ƒXåæ ‡ - å›¾æ–‘å‡ ä½•ä¸­å¿ƒçš„Xåæ ‡'
            },
            'centroid_y': {
                'type': 'float',
                'description': 'è´¨å¿ƒYåæ ‡ - å›¾æ–‘å‡ ä½•ä¸­å¿ƒçš„Yåæ ‡'
            },
            'area': {
                'type': 'float',
                'description': 'é¢ç§¯ - å›¾æ–‘è®¡ç®—é¢ç§¯ï¼ˆå¹³æ–¹ç±³ï¼‰'
            },
            'perimeter': {
                'type': 'float',
                'description': 'å‘¨é•¿ - å›¾æ–‘è¾¹ç•Œå‘¨é•¿ï¼ˆç±³ï¼‰'
            },
            'min_x': {
                'type': 'float',
                'description': 'æœ€å°Xåæ ‡ - å›¾æ–‘è¾¹ç•Œæ¡†çš„æœ€å°Xåæ ‡'
            },
            'min_y': {
                'type': 'float',
                'description': 'æœ€å°Yåæ ‡ - å›¾æ–‘è¾¹ç•Œæ¡†çš„æœ€å°Yåæ ‡'
            },
            'max_x': {
                'type': 'float',
                'description': 'æœ€å¤§Xåæ ‡ - å›¾æ–‘è¾¹ç•Œæ¡†çš„æœ€å¤§Xåæ ‡'
            },
            'max_y': {
                'type': 'float',
                'description': 'æœ€å¤§Yåæ ‡ - å›¾æ–‘è¾¹ç•Œæ¡†çš„æœ€å¤§Yåæ ‡'
            }
        }
        
        logger = get_logger(__name__)
        columns = []
        
        # ä¸ºå®é™…å­˜åœ¨çš„å­—æ®µç”Ÿæˆé…ç½®
        for col in actual_columns:
            if col in standard_fields:
                # ä½¿ç”¨æ ‡å‡†é…ç½®
                config = standard_fields[col].copy()
                config['name'] = col
                columns.append(config)
                logger.debug(f"âœ… ä½¿ç”¨æ ‡å‡†é…ç½®: {col}")
            else:
                # ä½¿ç”¨é»˜è®¤æ¨æ–­é€»è¾‘
                columns.append({
                    'name': col,
                    'type': 'string',  # é»˜è®¤ä¸ºå­—ç¬¦ä¸²ç±»å‹
                    'description': f'å­—æ®µ {col} - è‡ªåŠ¨æ¨æ–­å­—æ®µ'
                })
                logger.debug(f"âš ï¸ ä½¿ç”¨é»˜è®¤é…ç½®: {col}")
        
        logger.info(f"ğŸ“Š ç”Ÿæˆäº†{len(columns)}ä¸ªå­—æ®µé…ç½®ï¼Œå…¶ä¸­{sum(1 for col in actual_columns if col in standard_fields)}ä¸ªä½¿ç”¨æ ‡å‡†é…ç½®")
        
        return columns
    
    @staticmethod
    def clear_semantic_cache(dataset_path: str = None):
        """
        æ¸…ç†PandasAIè¯­ä¹‰å±‚ç¼“å­˜
        """
        logger = get_logger(__name__)
        
        try:
            # æ¸…ç†PandasAIç¼“å­˜ç›®å½•
            cache_dir = "datasets"
            if os.path.exists(cache_dir):
                import shutil
                if dataset_path:
                    # æ¸…ç†ç‰¹å®šæ•°æ®é›†çš„ç¼“å­˜
                    # å¤„ç†åµŒå¥—è·¯å¾„ï¼Œå¦‚ 'sementic/886e481d-7c9e-4310-be6e-82125c7b9f13'
                    if '/' in dataset_path:
                        # å¯¹äºåµŒå¥—è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨åŸè·¯å¾„æ„å»ºç¼“å­˜ç›®å½•
                        dataset_cache_dir = os.path.join(cache_dir, *dataset_path.split('/'))
                    else:
                        # å¯¹äºç®€å•è·¯å¾„ï¼Œä½¿ç”¨ä¸‹åˆ’çº¿æ›¿æ¢
                        dataset_cache_dir = os.path.join(cache_dir, dataset_path.replace("/", "_"))
                    
                    if os.path.exists(dataset_cache_dir):
                        shutil.rmtree(dataset_cache_dir)
                        logger.info(f"ğŸ§¹ å·²æ¸…ç†æ•°æ®é›†ç¼“å­˜: {dataset_path} -> {dataset_cache_dir}")
                    else:
                        logger.warning(f"âš ï¸ ç¼“å­˜ç›®å½•ä¸å­˜åœ¨: {dataset_cache_dir}")
                else:
                    # æ¸…ç†æ‰€æœ‰ç¼“å­˜
                    shutil.rmtree(cache_dir)
                    logger.info("ğŸ§¹ å·²æ¸…ç†æ‰€æœ‰PandasAIç¼“å­˜")
            
            # å°è¯•ä»PandasAIå†…éƒ¨æ¸…ç†æ•°æ®é›†æ³¨å†Œ
            try:
                if hasattr(pai, '_datasets') and dataset_path and dataset_path in pai._datasets:
                    del pai._datasets[dataset_path]
                    logger.info(f"ğŸ—‘ï¸ å·²ä»å†…å­˜æ¸…ç†æ•°æ®é›†: {dataset_path}")
            except:
                pass  # å¿½ç•¥å†…éƒ¨æ¸…ç†é”™è¯¯
                
        except Exception as e:
            logger.warning(f"âš ï¸ ç¼“å­˜æ¸…ç†å¤±è´¥: {str(e)}")
    
    @staticmethod
    def create_semantic_dataframe_from_config(file_path: str, dataset_id: str, db: SchemaDatabase = None) -> Any:
        """
        ä»æ•°æ®åº“ä¸­çš„é…ç½®åˆ›å»ºè¯­ä¹‰å±‚DataFrame
        
        Args:
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            dataset_id: æ•°æ®é›†IDï¼Œç”¨äºä»æ•°æ®åº“è·å–é…ç½®
            db: å¯é€‰çš„æ•°æ®åº“è¿æ¥å®ä¾‹ï¼Œå¦‚æœæä¾›åˆ™å¤ç”¨ï¼Œå¦åˆ™åˆ›å»ºæ–°è¿æ¥
        """
        logger = get_logger(__name__)
        
        # ä»æ•°æ®åº“è·å–è¯­ä¹‰é…ç½®
        try:
            # å¦‚æœæ²¡æœ‰æä¾›æ•°æ®åº“è¿æ¥ï¼Œåˆ™åˆ›å»ºæ–°çš„
            if db is None:
                settings = Settings()
                db = SchemaDatabase(settings)
                logger.debug("åˆ›å»ºäº†æ–°çš„æ•°æ®åº“è¿æ¥")
            else:
                logger.debug("å¤ç”¨ç°æœ‰çš„æ•°æ®åº“è¿æ¥")
                
            schema = db.get_dataset_schema(dataset_id)
            
            if not schema:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ•°æ®é›†é…ç½®: {dataset_id}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                schema = {"columns": []}
            else:
                logger.info(f"âœ… ä»æ•°æ®åº“è·å–åˆ°è¯­ä¹‰é…ç½®: {dataset_id}")
                logger.debug(f"ğŸ“‹ é…ç½®å†…å®¹: {schema}")
                
        except Exception as e:
            logger.error(f"âŒ ä»æ•°æ®åº“è·å–é…ç½®å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            schema = {"columns": []}
        
        # è¯»å–åŸå§‹æ•°æ®
        raw_df = DataAnalyzer.load_data(file_path)
        
        # ä½¿ç”¨pai.createåˆ›å»ºè¯­ä¹‰å±‚DataFrame
        columns_config = schema.get("columns", [])
        # ä½¿ç”¨ç¬¦åˆPandasAIè¦æ±‚çš„è·¯å¾„æ ¼å¼ï¼šorganization/dataset
        dataset_path = f"semantic/{dataset_id}"
        
        # å¦‚æœé…ç½®ä¸ºç©ºï¼Œç”Ÿæˆæ ‡å‡†é…ç½®
        if not columns_config:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„å­—æ®µé…ç½®ï¼Œç”Ÿæˆæ ‡å‡†é…ç½®")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºåœŸåœ°æ•°æ®
            is_land_data = any(field in raw_df.columns for field in ['ZLDWDM', 'DLBM', 'TBMJ', 'QSXZ'])
            
            if is_land_data:
                logger.info("ğŸï¸ æ£€æµ‹åˆ°åœŸåœ°æ•°æ®ï¼Œä½¿ç”¨æ ‡å‡†åœŸåœ°å­—æ®µé…ç½®")
                columns_config = DataAnalyzer._generate_standard_land_columns(list(raw_df.columns))
            else:
                # ä½¿ç”¨é»˜è®¤é…ç½®
                columns_config = [
                    {"name": col, "type": str(raw_df[col].dtype), "description": f"å­—æ®µ {col}"} 
                    for col in raw_df.columns
                ]
        
        logger.info(f"ğŸ”§ ä½¿ç”¨è¯­ä¹‰å±‚è·¯å¾„: {dataset_path}")
        logger.info(f"ğŸ“Š é…ç½®äº†{len(columns_config)}ä¸ªå­—æ®µçš„è¯­ä¹‰ä¿¡æ¯")
        
        # æ‰“å°è¯¦ç»†çš„åˆ—é…ç½®ä¿¡æ¯ï¼Œä¾¿äºè°ƒè¯•
        for i, col_config in enumerate(columns_config[:3]):  # åªæ‰“å°å‰3ä¸ª
            logger.debug(f"ğŸ” åˆ—é…ç½®{i+1}: {col_config}")
        
        try:
            # å…ˆæ¸…ç†å¯èƒ½å­˜åœ¨çš„ç¼“å­˜ï¼Œç„¶ååˆ›å»º
            DataAnalyzer.clear_semantic_cache(dataset_path)
            
            # åˆ›å»ºè¯­ä¹‰æ•°æ®æ¡†
            semantic_df = pai.create(
                path=dataset_path,
                df=pai.DataFrame(raw_df, config={"description": schema.get("description", "")}),
                description=schema.get("description", ""),
                columns=columns_config
            )
            logger.info("âœ… è¯­ä¹‰æ•°æ®æ¡†åˆ›å»ºæˆåŠŸ")
            return semantic_df
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºè¯­ä¹‰æ•°æ®æ¡†å¤±è´¥: {str(e)}")
            # å¦‚æœå¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹DataFrame
            logger.warning("ğŸ”„ å›é€€åˆ°åŸå§‹DataFrameæ¨¡å¼")
            return pai.DataFrame(raw_df)
    
    @staticmethod
    def create_semantic_dataframe(file_path: str, schema: Optional[Dict[str, Any]] = None) -> Any:
        """
        ä½¿ç”¨è¯­ä¹‰å±‚åˆ›å»ºPandasAI DataFrameï¼ˆè‡ªåŠ¨ç”Ÿæˆé…ç½®ï¼‰
        """
        logger = get_logger(__name__)
        
        # å¦‚æœæ²¡æœ‰æä¾›schemaï¼Œè‡ªåŠ¨ç”Ÿæˆ
        if schema is None:
            # ä»æ–‡ä»¶åæ¨æ–­æ•°æ®é›†åç§°ï¼Œç¡®ä¿ç¬¦åˆå‘½åè§„èŒƒ
            dataset_name = os.path.splitext(os.path.basename(file_path))[0]
            # æ ‡å‡†åŒ–æ•°æ®é›†åç§°
            dataset_name = re.sub(r'[^\w\s-]', '', dataset_name)
            dataset_name = re.sub(r'[\s_]+', '-', dataset_name).lower().strip('-')
            dataset_name = dataset_name if dataset_name else 'default-dataset'
            
            schema = DataAnalyzer.create_semantic_schema(file_path, dataset_name=dataset_name)
        
        # è¯»å–åŸå§‹æ•°æ®
        raw_df = DataAnalyzer.load_data(file_path)
        
        # ä½¿ç”¨pai.createåˆ›å»ºè¯­ä¹‰å±‚DataFrame
        columns_config = schema.get("columns", [])
        # ä½¿ç”¨ç¬¦åˆPandasAIè¦æ±‚çš„è·¯å¾„æ ¼å¼ï¼šorganization/dataset
        dataset_path = f"ask-data/{dataset_name}"
        
        # å¦‚æœé…ç½®ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        if not columns_config:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„å­—æ®µé…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            columns_config = [
                {"name": col, "type": str(raw_df[col].dtype), "description": f"å­—æ®µ {col}"} 
                for col in raw_df.columns
            ]
        
        logger.info(f"ğŸ”§ ä½¿ç”¨è¯­ä¹‰å±‚è·¯å¾„: {dataset_path}")
        logger.info(f"ğŸ“Š é…ç½®äº†{len(columns_config)}ä¸ªå­—æ®µçš„è¯­ä¹‰ä¿¡æ¯")
        
        try:
            # å…ˆæ¸…ç†å¯èƒ½å­˜åœ¨çš„ç¼“å­˜ï¼Œç„¶ååˆ›å»º
            DataAnalyzer.clear_semantic_cache(dataset_path)
            
            # åˆ›å»ºè¯­ä¹‰æ•°æ®æ¡†
            semantic_df = pai.create(
                path=dataset_path,
                df=pai.DataFrame(raw_df, config={"description": schema.get("description", "")}),
                description=schema.get("description", ""),
                columns=columns_config
            )
            logger.info("âœ… è¯­ä¹‰æ•°æ®æ¡†åˆ›å»ºæˆåŠŸ")
            return semantic_df
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºè¯­ä¹‰æ•°æ®æ¡†å¤±è´¥: {str(e)}")
            # å¦‚æœå¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹DataFrame
            logger.warning("ğŸ”„ å›é€€åˆ°åŸå§‹DataFrameæ¨¡å¼")
            return pai.DataFrame(raw_df)
    
    @staticmethod
    def generate_context(data_info):
        """æ ¹æ®æ•°æ®åˆ†æç»“æœç”Ÿæˆä¸Šä¸‹æ–‡ä¿¡æ¯"""
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ZLDWDMå­—æ®µï¼ˆåœŸåœ°æ•°æ®çš„æ ‡è¯†ç ï¼‰
        geo_context = ""
        if 'ZLDWDM' in data_info['columns']:
            geo_context = """
åœ°ç†ä½ç½®è¯†åˆ«è¯´æ˜ï¼š
- ZLDWDMå­—æ®µæ˜¯æ ‡è¯†ç ï¼ŒåŒ…å«å®Œæ•´çš„è¡Œæ”¿åŒºåˆ’å±‚çº§ä¿¡æ¯
- è¡Œæ”¿åŒºåˆ’ä»£ç ç»“æ„ï¼š
  * å‰2ä½ï¼šçœçº§ä»£ç ï¼ˆ36 = æ±Ÿè¥¿çœï¼‰
  * å‰4ä½ï¼šå¸‚çº§ä»£ç ï¼ˆ3607 = èµ£å·å¸‚ï¼‰
  * å‰6ä½ï¼šå¿çº§ä»£ç 
  * å‰12ä½ï¼šæ‘çº§ä»£ç 

- æŸ¥è¯¢è¯­æ³•ç¤ºä¾‹ï¼š
  * æŸ¥è¯¢çœçº§ï¼šWHERE SUBSTRING(CAST(ZLDWDM AS VARCHAR), 1, 2) = '36'
  * æŸ¥è¯¢å¸‚çº§ï¼šWHERE SUBSTRING(CAST(ZLDWDM AS VARCHAR), 1, 4) = '3607'
  * æŸ¥è¯¢å¿çº§ï¼šWHERE SUBSTRING(CAST(ZLDWDM AS VARCHAR), 1, 6) = '360726'
  
"""
        
        context = f"""
æ•°æ®ç»“æ„ä¿¡æ¯ï¼š
- æ•°æ®è§„æ¨¡ï¼š{data_info['shape'][0]}è¡Œï¼Œ{data_info['shape'][1]}åˆ—
- æ—¥æœŸåˆ—ï¼š{', '.join(data_info['date_columns'])} ({data_info.get(data_info['date_columns'][0] + '_range', 'æœªçŸ¥èŒƒå›´') if data_info['date_columns'] else 'æ— '})
- åˆ†ç±»åˆ—ä¿¡æ¯ï¼š{json.dumps(data_info['categorical_info'], ensure_ascii=False, indent=2)}
{geo_context}
æŸ¥è¯¢è¦æ±‚ï¼š
1. å¿…é¡»ä½¿ç”¨execute_sql_queryå‡½æ•°
2. ä»”ç»†åˆ†æç”¨æˆ·é—®é¢˜ï¼Œç¡®ä¿æŸ¥è¯¢é€»è¾‘æ­£ç¡®
3. å¯¹äºæ¯”è¾ƒåˆ†æï¼Œéœ€è¦åˆ†åˆ«æŸ¥è¯¢ä¸åŒæ¡ä»¶çš„æ•°æ®
4. æ³¨æ„æ—¥æœŸæ ¼å¼å’Œå­—æ®µåç§°çš„å‡†ç¡®æ€§
5. æ³¨æ„è€ƒè™‘æ•°æ®ä¸­æ²¡ç”¨æˆ·æƒ³è¦çš„æ•°æ®çš„æƒ…å†µï¼Œæ¯”å¦‚ç”¨æˆ·æƒ³è¦æŸ¥è¯¢2010çš„æ•°æ®ï¼Œä½†æ˜¯æ•°æ®ä¸­æ²¡æœ‰2010çš„åœŸåœ°æ•°æ®ï¼Œé‚£ä¹ˆéœ€è¦å‘Šè¯‰ç”¨æˆ·æ•°æ®ä¸­æ²¡æœ‰2010çš„åœŸåœ°æ•°æ®
"""
        return context
    
    # ====================== Transformations æ”¯æŒæ–¹æ³• ======================
    
    @staticmethod
    def auto_generate_transformations(df: pd.DataFrame, data_type: str = "auto") -> List[Dict[str, Any]]:
        """
        ä¸ºDataFrameè‡ªåŠ¨ç”Ÿæˆtransformationsé…ç½®
        
        Args:
            df: pandas DataFrame
            data_type: æ•°æ®ç±»å‹æç¤º
            
        Returns:
            ç”Ÿæˆçš„transformationsé…ç½®åˆ—è¡¨
        """
        logger = get_logger(__name__)
        
        try:
            # ä½¿ç”¨æ–°çš„è½¬æ¢è§„åˆ™ç®¡ç†å™¨
            from ..utils.transformation_rules import get_rule_manager
            
            rule_manager = get_rule_manager()
            transformations = rule_manager.generate_transformations_for_dataframe(df)
            
            if transformations:
                logger.info(f"âœ… ä½¿ç”¨è§„åˆ™ç®¡ç†å™¨ç”Ÿæˆäº†{len(transformations)}ä¸ªæ•°æ®è½¬æ¢é…ç½®")
                return transformations
            else:
                # å¦‚æœæ²¡æœ‰åŒ¹é…çš„è§„åˆ™ï¼Œä½¿ç”¨é€šç”¨çš„transformationsç”Ÿæˆé€»è¾‘ä½œä¸ºå›é€€
                logger.warning("âš ï¸ æ²¡æœ‰åŒ¹é…çš„è½¬æ¢è§„åˆ™ï¼Œä½¿ç”¨é€šç”¨ç”Ÿæˆé€»è¾‘")
                transformations = TransformationsHelper.generate_transformations_for_data(df, data_type)
                logger.info(f"âœ… è‡ªåŠ¨ç”Ÿæˆ{len(transformations)}ä¸ªæ•°æ®è½¬æ¢é…ç½®")
                return transformations
        except Exception as e:
            logger.error(f"âŒ è‡ªåŠ¨ç”Ÿæˆtransformationså¤±è´¥: {e}")
            return []
    
# æ³¨æ„ï¼šåŸæœ‰çš„ _generate_land_data_transformations æ–¹æ³•å·²è¢«æ–°çš„è§„åˆ™ç®¡ç†å™¨æ›¿ä»£
    # ç›¸å…³é…ç½®ç°åœ¨å­˜å‚¨åœ¨ utils/transformation_rules.json æ–‡ä»¶ä¸­
    
    @staticmethod
    def apply_transformations_to_dataframe(df: pd.DataFrame, transformations: List[Dict[str, Any]]) -> pd.DataFrame:
        """
        å°†transformationsåº”ç”¨åˆ°DataFrame
        
        Args:
            df: åŸå§‹DataFrame
            transformations: transformationsé…ç½®åˆ—è¡¨
            
        Returns:
            åº”ç”¨transformationsåçš„DataFrame
        """
        logger = get_logger(__name__)
        result_df = df.copy()
        
        for i, transformation in enumerate(transformations):
            if not transformation.get('enabled', True):
                continue
                
            try:
                trans_type = transformation['type']
                params = transformation.get('params', {})
                
                logger.debug(f"åº”ç”¨è½¬æ¢ {i+1}: {transformation.get('name', trans_type)}")
                
                # æ ¹æ®è½¬æ¢ç±»å‹åº”ç”¨ç›¸åº”çš„æ“ä½œ
                if trans_type == 'extract':
                    result_df = DataAnalyzer._apply_extract(result_df, params)
                elif trans_type == 'to_numeric':
                    result_df = DataAnalyzer._apply_to_numeric(result_df, params)
                elif trans_type == 'ensure_positive':
                    result_df = DataAnalyzer._apply_ensure_positive(result_df, params)
                elif trans_type == 'map_values':
                    result_df = DataAnalyzer._apply_map_values(result_df, params)
                elif trans_type == 'round_numbers':
                    result_df = DataAnalyzer._apply_round_numbers(result_df, params)
                elif trans_type == 'strip':
                    result_df = DataAnalyzer._apply_strip(result_df, params)
                elif trans_type == 'fill_na':
                    result_df = DataAnalyzer._apply_fill_na(result_df, params)
                elif trans_type == 'format_date':
                    result_df = DataAnalyzer._apply_format_date(result_df, params)
                # å¯ä»¥ç»§ç»­æ·»åŠ æ›´å¤šè½¬æ¢ç±»å‹çš„æ”¯æŒ
                else:
                    logger.warning(f"âš ï¸ æš‚ä¸æ”¯æŒçš„è½¬æ¢ç±»å‹: {trans_type}")
                    
            except Exception as e:
                logger.error(f"âŒ åº”ç”¨è½¬æ¢å¤±è´¥ {transformation.get('name', trans_type)}: {e}")
                continue
        
        return result_df
    
    @staticmethod
    def _apply_extract(df: pd.DataFrame, params: Dict[str, Any]) -> pd.DataFrame:
        """åº”ç”¨extractè½¬æ¢"""
        column = params['column']
        pattern = params['pattern']
        new_column = params.get('new_column', f"{column}_extracted")
        
        if column in df.columns:
            df[new_column] = df[column].astype(str).str.extract(pattern, expand=False)
        
        return df
    
    @staticmethod
    def _apply_format_date(df: pd.DataFrame, params: Dict[str, Any]) -> pd.DataFrame:
        """åº”ç”¨format_dateè½¬æ¢"""
        column = params['column']
        format_str = params.get('format', '%Y-%m-%d')
        new_column = params.get('new_column', column)  # é»˜è®¤è¦†ç›–åŸåˆ—ï¼Œé™¤éæŒ‡å®šæ–°åˆ—å
        
        if column in df.columns:
            # ç¡®ä¿åˆ—æ˜¯datetimeç±»å‹
            if not pd.api.types.is_datetime64_any_dtype(df[column]):
                # å¦‚æœä¸æ˜¯datetimeç±»å‹ï¼Œå…ˆå°è¯•è½¬æ¢
                try:
                    df[column] = pd.to_datetime(df[column], errors='coerce')
                except:
                    logger = get_logger(__name__)
                    logger.warning(f"âš ï¸ æ— æ³•å°†åˆ— {column} è½¬æ¢ä¸ºdatetimeç±»å‹")
                    return df
            
            # åº”ç”¨æ—¥æœŸæ ¼å¼åŒ–
            df[new_column] = df[column].dt.strftime(format_str)
        
        return df
    
    @staticmethod
    def _apply_to_numeric(df: pd.DataFrame, params: Dict[str, Any]) -> pd.DataFrame:
        """åº”ç”¨to_numericè½¬æ¢"""
        column = params['column']
        errors = params.get('errors', 'coerce')
        
        if column in df.columns:
            df[column] = pd.to_numeric(df[column], errors=errors)
        
        return df
    
    @staticmethod
    def _apply_ensure_positive(df: pd.DataFrame, params: Dict[str, Any]) -> pd.DataFrame:
        """åº”ç”¨ensure_positiveè½¬æ¢"""
        column = params['column']
        drop_negative = params.get('drop_negative', False)
        
        if column in df.columns:
            if drop_negative:
                df = df[df[column] >= 0]
            else:
                df[column] = df[column].abs()
        
        return df
    
    @staticmethod
    def _apply_map_values(df: pd.DataFrame, params: Dict[str, Any]) -> pd.DataFrame:
        """åº”ç”¨map_valuesè½¬æ¢"""
        column = params['column']
        mapping = params['mapping']
        new_column = params.get('new_column', f"{column}_mapped")
        
        if column in df.columns:
            df[new_column] = df[column].map(mapping).fillna(df[column])
        
        return df
    
    @staticmethod
    def _apply_round_numbers(df: pd.DataFrame, params: Dict[str, Any]) -> pd.DataFrame:
        """åº”ç”¨round_numbersè½¬æ¢"""
        column = params['column']
        decimals = params.get('decimals', 2)
        
        if column in df.columns and pd.api.types.is_numeric_dtype(df[column]):
            df[column] = df[column].round(decimals)
        
        return df
    
    @staticmethod
    def _apply_strip(df: pd.DataFrame, params: Dict[str, Any]) -> pd.DataFrame:
        """åº”ç”¨stripè½¬æ¢"""
        column = params['column']
        
        if column in df.columns and df[column].dtype == 'object':
            df[column] = df[column].astype(str).str.strip()
        
        return df
    
    @staticmethod
    def _apply_fill_na(df: pd.DataFrame, params: Dict[str, Any]) -> pd.DataFrame:
        """åº”ç”¨fill_naè½¬æ¢"""
        column = params['column']
        value = params['value']
        
        if column in df.columns:
            df[column] = df[column].fillna(value)
        
        return df
    
    @staticmethod
    def create_enhanced_semantic_dataframe(file_path: str, dataset_id: str, 
                                         db: SchemaDatabase = None, 
                                         auto_transformations: bool = True) -> Any:
        """
        åˆ›å»ºå¢å¼ºçš„è¯­ä¹‰DataFrameï¼ŒåŒ…å«transformationsæ”¯æŒ
        
        Args:
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            dataset_id: æ•°æ®é›†ID
            db: æ•°æ®åº“è¿æ¥
            auto_transformations: æ˜¯å¦è‡ªåŠ¨ç”Ÿæˆtransformations
            
        Returns:
            å¢å¼ºçš„è¯­ä¹‰DataFrame
        """
        logger = get_logger(__name__)
        
        try:
            # 1. è¯»å–åŸå§‹æ•°æ®
            raw_df = DataAnalyzer.load_data(file_path)
            
            # 2. ä»æ•°æ®åº“è·å–schemaé…ç½®
            if db is None:
                settings = Settings()
                db = SchemaDatabase(settings)
            
            schema = db.get_dataset_schema(dataset_id)
            
            # 3. å¦‚æœéœ€è¦ï¼Œè‡ªåŠ¨ç”Ÿæˆtransformations
            if auto_transformations and (not schema or not schema.get('transformations')):
                logger.info("ğŸ”„ è‡ªåŠ¨ç”Ÿæˆtransformationsé…ç½®")
                auto_trans = DataAnalyzer.auto_generate_transformations(raw_df, "land")
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                if auto_trans:
                    db.save_transformations(dataset_id, auto_trans)
                    logger.info(f"âœ… å·²ä¿å­˜{len(auto_trans)}ä¸ªè‡ªåŠ¨ç”Ÿæˆçš„transformations")
                    
                    # é‡æ–°è·å–schemaï¼ˆåŒ…å«transformationsï¼‰
                    schema = db.get_dataset_schema(dataset_id)
            
            # 4. åº”ç”¨transformationsåˆ°DataFrame
            transformed_df = raw_df.copy()
            if schema and schema.get('transformations'):
                logger.info(f"ğŸ”§ åº”ç”¨{len(schema['transformations'])}ä¸ªtransformations")
                transformed_df = DataAnalyzer.apply_transformations_to_dataframe(
                    transformed_df, 
                    schema['transformations']
                )
                logger.info(f"âœ… Transformationsåº”ç”¨å®Œæˆï¼Œæ•°æ®å½¢çŠ¶: {transformed_df.shape}")
            
            # 5. æ„å»ºåˆ—é…ç½®
            columns_config = []
            if schema and schema.get("columns"):
                columns_config = schema["columns"]
            else:
                # ä½¿ç”¨é»˜è®¤é…ç½®
                logger.warning("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„å­—æ®µé…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                for col in transformed_df.columns:
                    dtype = str(transformed_df[col].dtype)
                    if 'int' in dtype:
                        col_type = "integer"
                    elif 'float' in dtype:
                        col_type = "float"
                    elif 'datetime' in dtype:
                        col_type = "datetime"
                    elif 'bool' in dtype:
                        col_type = "boolean"
                    else:
                        col_type = "string"
                    
                    columns_config.append({
                        "name": col,
                        "type": col_type,
                        "description": f"å­—æ®µ {col}"
                    })
            
            # 6. æ„å»ºPandasAI v3æ ¼å¼çš„schema
            dataset_path = f"semantic/{dataset_id}"
            description = schema.get("description", f"æ•°æ®é›† {dataset_id}") if schema else f"æ•°æ®é›† {dataset_id}"
            
            # 7. æ¸…ç†ç¼“å­˜
            DataAnalyzer.clear_semantic_cache(dataset_path)
            
            # 8. ç›´æ¥ä½¿ç”¨å¤„ç†åçš„DataFrameåˆ›å»ºè¯­ä¹‰DataFrame
            logger.info(f"ğŸ“Š ä½¿ç”¨å¤„ç†åçš„æ•°æ®åˆ›å»ºè¯­ä¹‰DataFrameï¼Œå½¢çŠ¶: {transformed_df.shape}")
            
            # ç›´æ¥ä½¿ç”¨pai.DataFrameåˆ›å»ºï¼Œä¸ä½¿ç”¨pai.create
            semantic_df = pai.DataFrame(transformed_df)
            
            logger.info("âœ… å¢å¼ºè¯­ä¹‰æ•°æ®æ¡†åˆ›å»ºæˆåŠŸï¼ˆç›´æ¥ä½¿ç”¨å¤„ç†åçš„æ•°æ®ï¼‰")
            return semantic_df
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºå¢å¼ºè¯­ä¹‰æ•°æ®æ¡†å¤±è´¥: {e}")
            # å›é€€åˆ°åŸºç¡€æ¨¡å¼
            return DataAnalyzer.create_semantic_dataframe_from_config(file_path, dataset_id, db)
 