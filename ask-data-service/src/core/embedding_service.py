"""
EmbeddingæœåŠ¡æ¨¡å—

æ”¯æŒOpenAIåè®®çš„è¿œç¨‹embeddingæœåŠ¡
"""

import numpy as np
from typing import List, Union, Optional
import requests
import json
from ..utils.logger import get_logger

class EmbeddingService:
    """è¿œç¨‹embeddingæœåŠ¡"""
    
    def __init__(self, api_key: str, base_url: str, model_name: str = "bge-large-zh-v1.5"):
        """
        åˆå§‹åŒ–embeddingæœåŠ¡
        
        Args:
            api_key: APIå¯†é’¥ï¼ˆæ­¤æœåŠ¡å¯èƒ½ä¸éœ€è¦ï¼Œä½†ä¿ç•™æ¥å£å…¼å®¹æ€§ï¼‰
            base_url: APIåŸºç¡€URL
            model_name: æ¨¡å‹åç§°
        """
        self.logger = get_logger(__name__)
        self.model_name = model_name
        self.api_key = api_key
        self.base_url = base_url.rstrip('/')
        self.embeddings_url = f"{self.base_url}/embeddings"
        
        self.logger.info(f"åˆå§‹åŒ–è¿œç¨‹embeddingæœåŠ¡ - æ¨¡å‹: {model_name}, URL: {base_url}")
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.headers = {
            "Content-Type": "application/json"
        }
        
        # å¦‚æœæä¾›äº†APIå¯†é’¥ï¼Œæ·»åŠ åˆ°å¤´ä¿¡æ¯ä¸­ï¼ˆä½†å¯èƒ½ä¸éœ€è¦ï¼‰
        if api_key and api_key != "dummy":
            self.headers["Authorization"] = f"Bearer {api_key}"
        
        self.logger.info("è¿œç¨‹embeddingæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
    
    def encode_single(self, text: str, normalize: bool = True) -> np.ndarray:
        """
        å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œå‘é‡åŒ–
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            normalize: æ˜¯å¦å¯¹å‘é‡è¿›è¡Œå½’ä¸€åŒ–
            
        Returns:
            numpy.ndarray: æ–‡æœ¬å‘é‡
        """
        if not text or not text.strip():
            self.logger.warning("è¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼Œè¿”å›é›¶å‘é‡")
            return np.zeros(1024)  # å‡è®¾å‘é‡ç»´åº¦ä¸º1024
        
        try:
            # å‡†å¤‡è¯·æ±‚æ•°æ®
            data = {
                "model": self.model_name,
                "input": text.strip()
            }
            
            # è°ƒç”¨è¿œç¨‹embedding API
            response = requests.post(
                self.embeddings_url,
                headers=self.headers,
                json=data,
                timeout=30
            )
            
            if response.status_code != 200:
                raise Exception(f"HTTP {response.status_code}: {response.text}")
            
            # è§£æå“åº”
            result = response.json()
            embedding = np.array(result["data"][0]["embedding"])
            
            # å¦‚æœéœ€è¦å½’ä¸€åŒ–
            if normalize:
                norm = np.linalg.norm(embedding)
                if norm > 0:
                    embedding = embedding / norm
            
            return embedding
            
        except Exception as e:
            self.logger.error(f"è¿œç¨‹æ–‡æœ¬å‘é‡åŒ–å¤±è´¥: {e}")
            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {type(e).__name__}: {str(e)}")
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœåŠ¡ä¸å¯ç”¨çš„é”™è¯¯
            if "502" in str(e) or "Bad Gateway" in str(e):
                self.logger.error("EmbeddingæœåŠ¡ä¸å¯ç”¨ (502 Bad Gateway)ï¼Œè¯·æ£€æŸ¥æœåŠ¡çŠ¶æ€")
            return np.zeros(1024)
    
    def encode_batch(self, texts: List[str], normalize: bool = True, batch_size: int = 32) -> List[np.ndarray]:
        """
        æ‰¹é‡æ–‡æœ¬å‘é‡åŒ–
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            normalize: æ˜¯å¦å¯¹å‘é‡è¿›è¡Œå½’ä¸€åŒ–
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            List[np.ndarray]: å‘é‡åˆ—è¡¨
        """
        if not texts:
            return []
        
        # è¿‡æ»¤ç©ºæ–‡æœ¬
        valid_texts = [text.strip() if text else "" for text in texts]
        results = []
        
        try:
            # åˆ†æ‰¹å¤„ç†
            for i in range(0, len(valid_texts), batch_size):
                batch_texts = valid_texts[i:i + batch_size]
                
                # å‡†å¤‡è¯·æ±‚æ•°æ®
                data = {
                    "model": self.model_name,
                    "input": batch_texts
                }
                
                # è°ƒç”¨è¿œç¨‹embedding API
                response = requests.post(
                    self.embeddings_url,
                    headers=self.headers,
                    json=data,
                    timeout=60
                )
                
                if response.status_code != 200:
                    raise Exception(f"HTTP {response.status_code}: {response.text}")
                
                # è§£æå“åº”
                result = response.json()
                
                # æå–å‘é‡
                for item in result["data"]:
                    embedding = np.array(item["embedding"])
                    
                    # å¦‚æœéœ€è¦å½’ä¸€åŒ–
                    if normalize:
                        norm = np.linalg.norm(embedding)
                        if norm > 0:
                            embedding = embedding / norm
                    
                    results.append(embedding)
            
            return results
            
        except Exception as e:
            self.logger.error(f"æ‰¹é‡å‘é‡åŒ–å¤±è´¥: {e}")
            return [np.zeros(1024) for _ in texts]
    
    def compute_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
        
        Args:
            embedding1: å‘é‡1
            embedding2: å‘é‡2
            
        Returns:
            float: ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        try:
            # ç¡®ä¿å‘é‡æ˜¯numpyæ•°ç»„
            emb1 = np.array(embedding1)
            emb2 = np.array(embedding2)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
            return float(similarity)
        except Exception as e:
            self.logger.error(f"ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def generate_dataset_embedding(self, dataset_info: dict) -> np.ndarray:
        """
        ä¸ºæ•°æ®é›†ç”Ÿæˆç»¼åˆembedding
        
        Args:
            dataset_info: æ•°æ®é›†ä¿¡æ¯å­—å…¸
            
        Returns:
            np.ndarray: æ•°æ®é›†å‘é‡
        """
        try:
            self.logger.debug(f"ğŸ§  å¼€å§‹ç”Ÿæˆæ•°æ®é›†å‘é‡: {dataset_info.get('name', 'Unknown')}")
            
            # æ„å»ºæ•°æ®é›†çš„ç»¼åˆæè¿°æ–‡æœ¬
            text_parts = []
            
            # æ•°æ®é›†åç§°
            if dataset_info.get('name'):
                text_parts.append(f"æ•°æ®é›†åç§°ï¼š{dataset_info['name']}")
                self.logger.debug(f"ğŸ“ æ·»åŠ åç§°: {dataset_info['name']}")
            
            # æ•°æ®é›†æè¿°
            if dataset_info.get('description'):
                text_parts.append(f"æè¿°ï¼š{dataset_info['description']}")
                self.logger.debug(f"ğŸ“ æ·»åŠ æè¿°: {dataset_info['description'][:50]}...")
            
            # å…³é”®è¯
            if dataset_info.get('keywords'):
                keywords = dataset_info['keywords']
                if isinstance(keywords, list):
                    text_parts.append(f"å…³é”®è¯ï¼š{', '.join(keywords)}")
                    self.logger.debug(f"ğŸ“ æ·»åŠ å…³é”®è¯åˆ—è¡¨: {len(keywords)}ä¸ª")
                else:
                    text_parts.append(f"å…³é”®è¯ï¼š{keywords}")
                    self.logger.debug(f"ğŸ“ æ·»åŠ å…³é”®è¯å­—ç¬¦ä¸²: {keywords}")
            
            # ä¸šåŠ¡é¢†åŸŸ
            if dataset_info.get('domain'):
                text_parts.append(f"ä¸šåŠ¡é¢†åŸŸï¼š{dataset_info['domain']}")
                self.logger.debug(f"ğŸ“ æ·»åŠ ä¸šåŠ¡é¢†åŸŸ: {dataset_info['domain']}")
            
            # æ•°æ®æ‘˜è¦
            if dataset_info.get('data_summary'):
                text_parts.append(f"æ•°æ®æ‘˜è¦ï¼š{dataset_info['data_summary']}")
                self.logger.debug(f"ğŸ“ æ·»åŠ æ•°æ®æ‘˜è¦: {dataset_info['data_summary'][:50]}...")
            
            # åˆ—ä¿¡æ¯
            if dataset_info.get('columns_info'):
                text_parts.append(f"æ•°æ®å­—æ®µï¼š{dataset_info['columns_info']}")
                self.logger.debug(f"ğŸ“ æ·»åŠ åˆ—ä¿¡æ¯: {dataset_info['columns_info'][:50]}...")
            
            # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
            combined_text = " ".join(text_parts)
            
            if not combined_text.strip():
                self.logger.warning("âŒ æ•°æ®é›†ä¿¡æ¯ä¸ºç©ºï¼Œè¿”å›é›¶å‘é‡")
                return np.zeros(1024)
            
            self.logger.debug(f"ğŸ“ åˆå¹¶æ–‡æœ¬é•¿åº¦: {len(combined_text)} å­—ç¬¦")
            self.logger.debug(f"ğŸ“ åˆå¹¶æ–‡æœ¬é¢„è§ˆ: {combined_text[:100]}...")
            
            # è°ƒç”¨å‘é‡åŒ–æœåŠ¡
            embedding = self.encode_single(combined_text)
            
            if embedding is not None and hasattr(embedding, 'shape'):
                self.logger.debug(f"âœ… å‘é‡ç”ŸæˆæˆåŠŸ: ç»´åº¦={embedding.shape}")
            else:
                self.logger.warning(f"âš ï¸ å‘é‡ç”Ÿæˆå¼‚å¸¸: è¿”å›å€¼ç±»å‹={type(embedding)}")
            
            return embedding
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®é›†å‘é‡ç”Ÿæˆå¤±è´¥: {e}")
            self.logger.error(f"âŒ é”™è¯¯è¯¦æƒ…: {type(e).__name__}: {str(e)}")
            import traceback
            self.logger.error(f"âŒ å †æ ˆè·Ÿè¸ª: {traceback.format_exc()}")
            return np.zeros(1024)
    
    def get_model_info(self) -> dict:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Returns:
            dict: æ¨¡å‹ä¿¡æ¯
        """
        return {
            "model_name": self.model_name,
            "api_base": self.base_url,
            "embedding_dim": 1024,  # æ ¹æ®å®é™…æ¨¡å‹è°ƒæ•´
            "service_type": "remote_openai"
        } 