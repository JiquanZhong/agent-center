"""
æ™ºèƒ½æŸ¥è¯¢APIè·¯ç”±

åŸºäºæ„å›¾è¯†åˆ«çš„è‡ªåŠ¨æ•°æ®é›†åŒ¹é…å’ŒæŸ¥è¯¢
"""

from fastapi import APIRouter, Depends, HTTPException, status
from typing import List, Dict, Any, Optional
import time
import uuid
import os
import base64
from datetime import datetime

from ..models import StandardResponse
from ..dependencies import get_database, get_settings, get_llm
from ...config.settings import Settings
from ...utils.schema_database import SchemaDatabase
from ...core.llm_adapter import CustomOpenAI
from ...core.embedding_service import EmbeddingService
from ...core.vector_search_service import VectorSearchService
from ...core.intent_engine import IntentRecognitionEngine
from ...core.query_engine import QueryEngine
from ...utils.logger import get_logger, LogContext
from pydantic import BaseModel, Field

# æ•°æ®æ¨¡å‹å®šä¹‰
class SmartQueryRequest(BaseModel):
    """æ™ºèƒ½æŸ¥è¯¢è¯·æ±‚æ¨¡å‹"""
    question: str = Field(..., description="ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€é—®é¢˜", example="æ±Ÿè¥¿çœ2023å¹´çš„åœŸåœ°é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ")
    max_suggestions: int = Field(3, description="æœ€å¤šè¿”å›å‡ ä¸ªæ•°æ®é›†å»ºè®®", ge=1, le=10)
    auto_execute: bool = Field(True, description="æ˜¯å¦è‡ªåŠ¨ä½¿ç”¨æœ€ä½³åŒ¹é…æ‰§è¡ŒæŸ¥è¯¢")
    min_score: float = Field(0.3, description="æœ€å°åŒ¹é…åˆ†æ•°", ge=0.0, le=1.0)
    query_id: Optional[str] = Field(None, description="æŸ¥è¯¢IDï¼Œç”¨äºæ ‡è¯†æ­¤æ¬¡æŸ¥è¯¢")

class DatasetMatchResult(BaseModel):
    """æ•°æ®é›†åŒ¹é…ç»“æœæ¨¡å‹"""
    dataset_id: str = Field(..., description="æ•°æ®é›†ID")
    dataset_name: str = Field(..., description="æ•°æ®é›†åç§°")
    description: str = Field("", description="æ•°æ®é›†æè¿°")
    match_score: float = Field(..., description="åŒ¹é…åˆ†æ•°")
    match_reason: str = Field(..., description="åŒ¹é…åŸå› ")
    domain: Optional[str] = Field(None, description="ä¸šåŠ¡é¢†åŸŸ")
    keywords: List[str] = Field([], description="å…³é”®è¯")

class SmartQueryResponse(BaseModel):
    """æ™ºèƒ½æŸ¥è¯¢å“åº”æ¨¡å‹"""
    success: bool = Field(..., description="æ“ä½œæ˜¯å¦æˆåŠŸ")
    message: str = Field(..., description="å“åº”æ¶ˆæ¯")
    data: Dict[str, Any] = Field(..., description="æŸ¥è¯¢ç»“æœæ•°æ®")
    error: Optional[str] = Field(None, description="é”™è¯¯ä¿¡æ¯")
    timestamp: str = Field(..., description="å“åº”æ—¶é—´æˆ³")

router = APIRouter(prefix="/smart-query", tags=["æ™ºèƒ½æŸ¥è¯¢"])
logger = get_logger(__name__)

# å…¨å±€å˜é‡ç”¨äºç¼“å­˜æœåŠ¡å®ä¾‹
_embedding_service = None
_vector_service = None
_intent_engine = None

def check_and_encode_chart(query_id):
    """
    æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†å›¾è¡¨æ–‡ä»¶ï¼Œå¹¶è¿”å› base64 ç¼–ç 
    
    Args:
        query_id: æŸ¥è¯¢ID
        
    Returns:
        tuple: (have_chart: bool, chart_base64: str or None)
    """
    chart_path = os.path.join("exports", "charts", f"{query_id}.png")
    
    if os.path.exists(chart_path):
        try:
            with open(chart_path, "rb") as image_file:
                image_data = image_file.read()
                chart_base64 = base64.b64encode(image_data).decode('utf-8')
                return True, chart_base64
        except Exception as e:
            logger.warning(f"è¯»å–å›¾è¡¨æ–‡ä»¶å¤±è´¥: {str(e)}")
            return True, None  # æ–‡ä»¶å­˜åœ¨ä½†è¯»å–å¤±è´¥
    
    return False, None

def get_embedding_service(settings: Settings = Depends(get_settings)) -> EmbeddingService:
    """è·å–è¿œç¨‹embeddingæœåŠ¡å®ä¾‹"""
    global _embedding_service
    if _embedding_service is None:
        logger.info("åˆå§‹åŒ–è¿œç¨‹embeddingæœåŠ¡")
        embedding_config = settings.get_embedding_config()
        _embedding_service = EmbeddingService(
            api_key=settings.api_key,  # ä½¿ç”¨OpenAI API Keyä½œä¸ºè®¤è¯
            base_url=embedding_config["base_url"],
            model_name=embedding_config["model"]
        )
    return _embedding_service

def get_vector_service(settings: Settings = Depends(get_settings)) -> VectorSearchService:
    """è·å–ESå‘é‡æ£€ç´¢æœåŠ¡å®ä¾‹"""
    global _vector_service
    if _vector_service is None:
        logger.info("åˆå§‹åŒ–ESå‘é‡æ£€ç´¢æœåŠ¡")
        # ä»è®¾ç½®ä¸­è·å–ESé…ç½®
        es_config = settings.get_elasticsearch_config()
        _vector_service = VectorSearchService(es_config, settings.elasticsearch_index)
    return _vector_service

def get_intent_engine(
    embedding_service: EmbeddingService = Depends(get_embedding_service),
    vector_service: VectorSearchService = Depends(get_vector_service),
    db: SchemaDatabase = Depends(get_database)
) -> IntentRecognitionEngine:
    """è·å–æ„å›¾è¯†åˆ«å¼•æ“å®ä¾‹"""
    global _intent_engine
    if _intent_engine is None:
        logger.info("åˆå§‹åŒ–æ„å›¾è¯†åˆ«å¼•æ“")
        _intent_engine = IntentRecognitionEngine(
            embedding_service=embedding_service,
            vector_service=vector_service,
            db=db
        )
    return _intent_engine

@router.post("/query", response_model=SmartQueryResponse, summary="æ™ºèƒ½æ•°æ®æŸ¥è¯¢")
async def smart_query(
    request: SmartQueryRequest,
    intent_engine: IntentRecognitionEngine = Depends(get_intent_engine),
    db: SchemaDatabase = Depends(get_database),
    settings: Settings = Depends(get_settings),
    llm: CustomOpenAI = Depends(get_llm)
):
    """
    æ™ºèƒ½æ•°æ®æŸ¥è¯¢ï¼šæ ¹æ®ç”¨æˆ·é—®é¢˜è‡ªåŠ¨è¯†åˆ«æ•°æ®é›†å¹¶æ‰§è¡ŒæŸ¥è¯¢
    
    å·¥ä½œæµç¨‹ï¼š
    1. åˆ†æç”¨æˆ·é—®é¢˜ï¼Œè¯†åˆ«æ„å›¾
    2. åŸºäºå‘é‡ç›¸ä¼¼åº¦åŒ¹é…æ•°æ®é›†
    3. è¿”å›åŒ¹é…ç»“æœæˆ–è‡ªåŠ¨æ‰§è¡ŒæŸ¥è¯¢
    """
    start_time = time.time()
    
    # ä½¿ç”¨ä¼ å…¥çš„ query_idï¼Œå¦‚æœæ²¡æœ‰æä¾›åˆ™ç”Ÿæˆä¸€ä¸ª
    query_id = request.query_id or str(uuid.uuid4())
    
    logger.info(f"ğŸ¤– æ™ºèƒ½æŸ¥è¯¢å¼€å§‹: {request.question}")
    logger.info(f"ğŸ” æŸ¥è¯¢ID: {query_id}")
    
    try:
        with LogContext(logger, "æ™ºèƒ½æŸ¥è¯¢å¤„ç†"):
            # 1. æ„å›¾è¯†åˆ«å’Œæ•°æ®é›†åŒ¹é…
            matches = intent_engine.recognize_intent(
                question=request.question,
                max_results=request.max_suggestions,
                min_score=request.min_score
            )
            
            # è®°å½•åŒ¹é…è¯¦æƒ…
            logger.info(f"ğŸ¯ æ„å›¾è¯†åˆ«ç»“æœ: å…±æ‰¾åˆ°{len(matches)}ä¸ªç¬¦åˆæ¡ä»¶çš„æ•°æ®é›† (min_score={request.min_score})")
            for i, match in enumerate(matches[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                logger.info(f"  [{i+1}] {match['dataset_name']} - åˆ†æ•°: {match['enhanced_score']:.3f}")
            
            # 2. è½¬æ¢åŒ¹é…ç»“æœæ ¼å¼
            matched_datasets = []
            for match in matches:
                dataset_match = DatasetMatchResult(
                    dataset_id=match["dataset_id"],
                    dataset_name=match["dataset_name"],
                    description=match.get("description", ""),
                    match_score=match["enhanced_score"],
                    match_reason=match["match_reason"],
                    domain=match.get("domain"),
                    keywords=match.get("keywords", [])
                )
                matched_datasets.append(dataset_match)
            
            # 3. å‡†å¤‡åŸºç¡€å“åº”æ•°æ®
            response_data = {
                "result": None,
                "result_type": "string",
                "execution_time": 0.0,
                "query_id": query_id,
                "have_chart": False,
                "chart_base64": None,
                "dataset_info": None,
                "matched_datasets": [match.dict() for match in matched_datasets],
                "auto_executed": False,
                "sql_queries": None
            }
            
            # 4. å¦‚æœå¯ç”¨è‡ªåŠ¨æ‰§è¡Œä¸”æœ‰åŒ¹é…ç»“æœ
            if request.auto_execute and matched_datasets:
                best_match = matched_datasets[0]
                
                # æ£€æŸ¥æœ€ä½³åŒ¹é…çš„åˆ†æ•°æ˜¯å¦è¶³å¤Ÿé«˜
                if best_match.match_score >= 0:
                    try:
                        logger.info(f"ğŸ¯ è‡ªåŠ¨æ‰§è¡ŒæŸ¥è¯¢ - æ•°æ®é›†: {best_match.dataset_name} (ID: {best_match.dataset_id})")
                        
                        # æ‰§è¡ŒæŸ¥è¯¢ - å¤ç”¨queryçš„æŸ¥è¯¢é€»è¾‘
                        query_result = await _execute_query_with_dataset(
                            dataset_id=best_match.dataset_id,
                            question=request.question,
                            query_id=query_id,
                            db=db,
                            settings=settings,
                            llm=llm
                        )
                        
                        # æ›´æ–°å“åº”æ•°æ®
                        response_data.update({
                            "result": query_result["result"],
                            "result_type": query_result["result_type"],
                            "have_chart": query_result["have_chart"],
                            "chart_base64": query_result["chart_base64"],
                            "dataset_info": query_result["dataset_info"],
                            "auto_executed": True,
                            "sql_queries": query_result.get("sql_queries")
                        })
                        
                        # è®°å½•æŸ¥è¯¢å·¥ä½œæµåˆ°æ•°æ®åº“
                        try:
                            workflow_recorded = db.record_query_workflow(
                                work_flow_run_id=query_id,
                                query_text=request.question,
                                dataset_id=best_match.dataset_id
                            )
                            if workflow_recorded:
                                logger.info(f"âœ… æŸ¥è¯¢å·¥ä½œæµè®°å½•æˆåŠŸ: {query_id} -> æ•°æ®é›† {best_match.dataset_id}")
                            else:
                                logger.warning(f"âš ï¸ æŸ¥è¯¢å·¥ä½œæµè®°å½•å¤±è´¥: {query_id}")
                        except Exception as wf_e:
                            logger.error(f"âŒ è®°å½•æŸ¥è¯¢å·¥ä½œæµå¼‚å¸¸: {wf_e}")
                        
                        message = f"æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸ"
                        
                    except Exception as e:
                        logger.error(f"è‡ªåŠ¨æ‰§è¡ŒæŸ¥è¯¢å¤±è´¥: {e}")
                        response_data["result"] = f"æ‰¾åˆ°åŒ¹é…æ•°æ®é›†ï¼Œä½†è‡ªåŠ¨æ‰§è¡ŒæŸ¥è¯¢å¤±è´¥: {str(e)}"
                        message = "æŸ¥è¯¢æ‰§è¡Œå¤±è´¥"
                
                else:
                    response_data["result"] = f"æ‰¾åˆ°åŒ¹é…æ•°æ®é›†ï¼Œä½†ç½®ä¿¡åº¦è¾ƒä½({best_match.match_score:.2f})ï¼Œå»ºè®®æ‰‹åŠ¨ç¡®è®¤"
                    message = "æ™ºèƒ½æŸ¥è¯¢å®Œæˆ"
            
            elif not matched_datasets:
                response_data["result"] = "æœªæ‰¾åˆ°åŒ¹é…çš„æ•°æ®é›†ï¼Œè¯·å°è¯•è°ƒæ•´é—®é¢˜æˆ–æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²åŒæ­¥"
                message = "æœªæ‰¾åˆ°åŒ¹é…æ•°æ®é›†"
            else:
                response_data["result"] = f"æ‰¾åˆ°{len(matched_datasets)}ä¸ªåŒ¹é…çš„æ•°æ®é›†"
                message = "æ™ºèƒ½æŸ¥è¯¢å®Œæˆ"
            
            response_data["execution_time"] = time.time() - start_time
            
            logger.info(f"âœ… æ™ºèƒ½æŸ¥è¯¢å®Œæˆ: æ‰¾åˆ°{len(matched_datasets)}ä¸ªåŒ¹é…, è‡ªåŠ¨æ‰§è¡Œ: {response_data['auto_executed']}")
            
            return SmartQueryResponse(
                success=True,
                message=message,
                data=response_data,
                timestamp=datetime.utcnow().isoformat()
            )
            
    except Exception as e:
        execution_time = time.time() - start_time
        logger.error(f"âŒ æ™ºèƒ½æŸ¥è¯¢å¤±è´¥: {e}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å›¾è¡¨ç”Ÿæˆ
        have_chart, chart_base64 = check_and_encode_chart(query_id)
        
        return SmartQueryResponse(
            success=False,
            message="æŸ¥è¯¢æ‰§è¡Œå¤±è´¥",
            data={
                "result": f"æ™ºèƒ½æŸ¥è¯¢å¤±è´¥: {str(e)}",
                "result_type": "string",
                "execution_time": execution_time,
                "query_id": query_id,
                "have_chart": have_chart,
                "chart_base64": chart_base64,
                "dataset_info": None,
                "matched_datasets": [],
                "auto_executed": False,
                "sql_queries": None
            },
            error=str(e),
            timestamp=datetime.utcnow().isoformat()
        )

async def _execute_query_with_dataset(
    dataset_id: str,
    question: str,
    query_id: str,
    db: SchemaDatabase,
    settings: Settings,
    llm: CustomOpenAI
) -> Dict[str, Any]:
    """
    ä½¿ç”¨æŒ‡å®šæ•°æ®é›†æ‰§è¡ŒæŸ¥è¯¢ - å¤ç”¨queryæ¥å£çš„å®Œæ•´é€»è¾‘
    
    Args:
        dataset_id: æ•°æ®é›†ID
        question: ç”¨æˆ·é—®é¢˜
        query_id: æŸ¥è¯¢ID
        db: æ•°æ®åº“æœåŠ¡
        settings: é…ç½®
        llm: è¯­è¨€æ¨¡å‹
        
    Returns:
        Dict: æŸ¥è¯¢ç»“æœ
    """
    # 1. è·å–æ•°æ®é›†ä¿¡æ¯
    dataset_info = db.get_dataset_by_id(dataset_id)
    if not dataset_info:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"æ•°æ®é›†ä¸å­˜åœ¨: {dataset_id}"
        )
    
    # 2. éªŒè¯æ•°æ®æ–‡ä»¶
    actual_data_path = dataset_info.get('actual_data_path')
    file_path = actual_data_path if actual_data_path else dataset_info['file_path']
    
    if not os.path.exists(file_path):
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
        )
    
    # è®°å½•æ–‡ä»¶è½¬æ¢ä¿¡æ¯
    if dataset_info.get('is_converted'):
        logger.info(f"ğŸ“„ æ£€æµ‹åˆ°æ–‡ä»¶è½¬æ¢: {dataset_info.get('original_file_type')} -> {dataset_info.get('data_file_type')}")
        logger.info(f"ğŸ—‚ï¸ åŸå§‹æ–‡ä»¶: {dataset_info.get('file_path')}")
        logger.info(f"ğŸ“Š æ•°æ®æ–‡ä»¶: {file_path}")
    else:
        logger.info(f"ğŸ“ æ•°æ®æ–‡ä»¶: {file_path}")
    
    # 3. åŠ è½½schemaé…ç½®
    schema_config = db.get_dataset_schema(dataset_id)
    if not schema_config:
        logger.warning("æœªæ‰¾åˆ°schemaé…ç½®ï¼Œå°†ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆæ¨¡å¼")
    
    # 4. åˆ›å»ºæŸ¥è¯¢å¼•æ“å¹¶æ‰§è¡ŒæŸ¥è¯¢
    query_engine = QueryEngine(
        llm=llm,
        data_path=file_path,
        use_semantic_layer=True if schema_config else False,
        settings=settings,
        dataset_id=dataset_id,
        schema_config=schema_config,
        db=db
    )
    
    # æ‰§è¡ŒæŸ¥è¯¢
    result = query_engine.query(question, query_id=query_id)
    
    # ç¡®å®šç»“æœç±»å‹
    result_type = "string"
    if isinstance(result, (int, float)):
        result_type = "number"
    elif hasattr(result, 'to_dict') and not isinstance(result, str):  # DataFrame
        result_type = "dataframe"
        try:
            import pandas as pd
            if isinstance(result, pd.DataFrame):
                result = result.to_dict('records')
            else:
                result = str(result)
        except Exception:
            result = str(result)
    elif isinstance(result, str) and any(keyword in result.lower() for keyword in ['å›¾', 'chart', 'plot']):
        result_type = "chart"
    
    # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†å›¾è¡¨å¹¶è·å– base64 ç¼–ç 
    have_chart, chart_base64 = check_and_encode_chart(query_id)
    
    return {
        "result": result,
        "result_type": result_type,
        "have_chart": have_chart,
        "sql_queries": query_engine.get_executed_sqls_string(),
        "chart_base64": chart_base64,
        "dataset_info": {
            "dataset_id": dataset_id,
            "dataset_name": dataset_info.get('name'),
            "file_path": file_path,
            "row_count": query_engine.get_data_info().get('shape', [0, 0])[0],
            "column_count": query_engine.get_data_info().get('shape', [0, 0])[1]
        },
    }

@router.post("/sync-datasets", response_model=StandardResponse, summary="åŒæ­¥æ•°æ®é›†åˆ°å‘é‡åº“")
async def sync_datasets_to_vector_store(
    force_refresh: bool = False,
    intent_engine: IntentRecognitionEngine = Depends(get_intent_engine)
):
    """
    åŒæ­¥æ•°æ®é›†åˆ°å‘é‡å­˜å‚¨
    
    Args:
        force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°æ‰€æœ‰æ•°æ®
    """
    try:
        with LogContext(logger, "åŒæ­¥æ•°æ®é›†åˆ°å‘é‡åº“"):
            sync_result = intent_engine.sync_datasets_to_vector_store(force_refresh)
            
            return StandardResponse(
                success=True,
                message=f"æ•°æ®é›†åŒæ­¥å®Œæˆ: æˆåŠŸ{sync_result['success_count']}ä¸ª, å¤±è´¥{sync_result['failed_count']}ä¸ª",
                data=sync_result,
                timestamp=datetime.utcnow().isoformat()
            )
            
    except Exception as e:
        logger.error(f"æ•°æ®é›†åŒæ­¥å¤±è´¥: {e}")
        return StandardResponse(
            success=False,
            message=f"æ•°æ®é›†åŒæ­¥å¤±è´¥: {str(e)}",
            error=str(e),
            timestamp=datetime.utcnow().isoformat()
        )

@router.get("/vector-stats", response_model=StandardResponse, summary="è·å–å‘é‡åº“ç»Ÿè®¡ä¿¡æ¯")
async def get_vector_stats(
    vector_service: VectorSearchService = Depends(get_vector_service)
):
    """è·å–å‘é‡åº“ç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = vector_service.get_stats()
        
        return StandardResponse(
            success=True,
            message="è·å–å‘é‡åº“ç»Ÿè®¡ä¿¡æ¯æˆåŠŸ",
            data=stats,
            timestamp=datetime.utcnow().isoformat()
        )
    except Exception as e:
        logger.error(f"è·å–å‘é‡åº“ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        return StandardResponse(
            success=False,
            message=f"è·å–å‘é‡åº“ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}",
            error=str(e),
            timestamp=datetime.utcnow().isoformat()
        )

@router.get("/embedding-info", response_model=StandardResponse, summary="è·å–embeddingæ¨¡å‹ä¿¡æ¯")
async def get_embedding_info(
    embedding_service: EmbeddingService = Depends(get_embedding_service)
):
    """è·å–embeddingæ¨¡å‹ä¿¡æ¯"""
    try:
        model_info = embedding_service.get_model_info()
        
        return StandardResponse(
            success=True,
            message="è·å–embeddingæ¨¡å‹ä¿¡æ¯æˆåŠŸ",
            data=model_info,
            timestamp=datetime.utcnow().isoformat()
        )
    except Exception as e:
        logger.error(f"è·å–embeddingæ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
        return StandardResponse(
            success=False,
            message=f"è·å–embeddingæ¨¡å‹ä¿¡æ¯å¤±è´¥: {str(e)}",
            error=str(e),
            timestamp=datetime.utcnow().isoformat()
        )



 