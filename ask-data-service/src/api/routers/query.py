"""
æŸ¥è¯¢APIè·¯ç”± - ç®€åŒ–ç‰ˆ

åªéœ€è¦dataset_idå³å¯æŸ¥è¯¢ï¼Œç³»ç»Ÿè‡ªåŠ¨ç®¡ç†æ–‡ä»¶è·¯å¾„å’Œé…ç½®
"""

from fastapi import APIRouter, Depends, HTTPException, status
from typing import Optional
import time
import uuid
import traceback
import os
import base64
from datetime import datetime

from ..models import QueryRequest, QueryResponse, StandardResponse
from ..dependencies import get_database, get_settings, get_llm
from ...core.query_engine import QueryEngine
from ...utils.schema_database import SchemaDatabase
from ...config.settings import Settings
from ...core.llm_adapter import CustomOpenAI
from ...utils.logger import get_logger, LogContext

# å¯¼å…¥å­—ä½“é…ç½®ï¼Œè§£å†³matplotlibä¸­æ–‡å­—ä½“é—®é¢˜
try:
    from ...utils.font_config import auto_configure
    auto_configure()  # è‡ªåŠ¨é…ç½®å­—ä½“
except ImportError as e:
    get_logger(__name__).warning(f"å­—ä½“é…ç½®æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

router = APIRouter(prefix="/query", tags=["æ•°æ®æŸ¥è¯¢"])
logger = get_logger(__name__)

def check_and_encode_chart(query_id):
    """
    æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†å›¾è¡¨æ–‡ä»¶ï¼Œå¹¶è¿”å› base64 ç¼–ç 
    
    Args:
        query_id: æŸ¥è¯¢ID
        
    Returns:
        tuple: (have_chart: bool, chart_base64: str or None)
    """
    chart_path = os.path.join("exports", "charts", f"{query_id}.png")
    
    if os.path.exists(chart_path):
        try:
            with open(chart_path, "rb") as image_file:
                image_data = image_file.read()
                chart_base64 = base64.b64encode(image_data).decode('utf-8')
                return True, chart_base64
        except Exception as e:
            logger.warning(f"è¯»å–å›¾è¡¨æ–‡ä»¶å¤±è´¥: {str(e)}")
            return True, None  # æ–‡ä»¶å­˜åœ¨ä½†è¯»å–å¤±è´¥
    
    return False, None

@router.post("/", response_model=QueryResponse, summary="æ‰§è¡Œæ•°æ®æŸ¥è¯¢")
async def execute_query(
    query_request: QueryRequest,
    db: SchemaDatabase = Depends(get_database),
    settings: Settings = Depends(get_settings),
    llm: CustomOpenAI = Depends(get_llm)
):
    """
    æ‰§è¡Œè‡ªç„¶è¯­è¨€æ•°æ®æŸ¥è¯¢
    
    åªéœ€è¦æä¾›dataset_idå’Œé—®é¢˜ï¼Œç³»ç»Ÿè‡ªåŠ¨ï¼š
    1. æ ¹æ®dataset_idæŸ¥æ‰¾æ•°æ®æ–‡ä»¶è·¯å¾„
    2. åŠ è½½å¯¹åº”çš„schemaé…ç½®
    3. æ‰§è¡ŒæŸ¥è¯¢å¹¶è¿”å›ç»“æœ
    """
    start_time = time.time()
    # ä½¿ç”¨ä¼ å…¥çš„ query_idï¼Œå¦‚æœæ²¡æœ‰æä¾›åˆ™ç”Ÿæˆä¸€ä¸ª
    query_id = query_request.query_id or str(uuid.uuid4())
    
    logger.info(f"ğŸ” å¼€å§‹æ‰§è¡ŒæŸ¥è¯¢ - ID: {query_id}")
    logger.info(f"ğŸ“‹ æ•°æ®é›†ID: {query_request.dataset_id}")
    logger.info(f"â“ æŸ¥è¯¢é—®é¢˜: {query_request.question}")
    
    try:
        # 1. æ ¹æ®dataset_idè·å–æ•°æ®é›†ä¿¡æ¯
        with LogContext(logger, f"è·å–æ•°æ®é›†ä¿¡æ¯: {query_request.dataset_id}"):
            dataset_info = db.get_dataset_by_id(query_request.dataset_id)
            
            if not dataset_info:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail=f"æ•°æ®é›†ä¸å­˜åœ¨: {query_request.dataset_id}"
                )
        
        # 2. éªŒè¯æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        # ä¼˜å…ˆä½¿ç”¨actual_data_pathï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨file_pathï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
        actual_data_path = dataset_info.get('actual_data_path')
        file_path = actual_data_path if actual_data_path else dataset_info['file_path']
        
        if not os.path.exists(file_path):
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
            )
        
        # è®°å½•æ–‡ä»¶è½¬æ¢ä¿¡æ¯
        if dataset_info.get('is_converted'):
            logger.info(f"ğŸ“„ æ£€æµ‹åˆ°æ–‡ä»¶è½¬æ¢: {dataset_info.get('original_file_type')} -> {dataset_info.get('data_file_type')}")
            logger.info(f"ğŸ—‚ï¸ åŸå§‹æ–‡ä»¶: {dataset_info.get('file_path')}")
            logger.info(f"ğŸ“Š æ•°æ®æ–‡ä»¶: {file_path}")
        else:
            logger.info(f"ğŸ“ æ•°æ®æ–‡ä»¶: {file_path}")
        
        # 3. åŠ è½½schemaé…ç½®
        with LogContext(logger, "åŠ è½½schemaé…ç½®"):
            schema_config = db.get_dataset_schema(query_request.dataset_id)
            if not schema_config:
                logger.warning("æœªæ‰¾åˆ°schemaé…ç½®ï¼Œå°†ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆæ¨¡å¼")
        
        # 4. åˆå§‹åŒ–æŸ¥è¯¢å¼•æ“
        with LogContext(logger, "åˆå§‹åŒ–æŸ¥è¯¢å¼•æ“"):
            query_engine = QueryEngine(
                llm=llm,
                data_path=file_path,
                use_semantic_layer=True if schema_config else False,
                settings=settings,
                dataset_id=query_request.dataset_id,
                schema_config=schema_config,
                db=db  # ä¼ é€’æ•°æ®åº“è¿æ¥ï¼Œé¿å…é‡å¤åˆ›å»º
            )
        
        # 5. æ‰§è¡ŒæŸ¥è¯¢
        with LogContext(logger, f"æ‰§è¡ŒæŸ¥è¯¢: {query_request.question}"):
            result = query_engine.query(query_request.question, query_id=query_id)
        
        execution_time = time.time() - start_time
        
        logger.info(f"âœ… æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸ - ID: {query_id}, è€—æ—¶: {execution_time:.2f}ç§’")
        
        # 6. ç¡®å®šç»“æœç±»å‹
        result_type = "string"
        if isinstance(result, (int, float)):
            result_type = "number"
        elif hasattr(result, 'to_dict') and not isinstance(result, str):  # DataFrame
            result_type = "dataframe"
            try:
                import pandas as pd
                if isinstance(result, pd.DataFrame):
                    result = result.to_dict('records')
                else:
                    result = str(result)
            except Exception:
                result = str(result)
        elif isinstance(result, str) and any(keyword in result.lower() for keyword in ['å›¾', 'chart', 'plot']):
            result_type = "chart"
        
        # 7. æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†å›¾è¡¨å¹¶è·å– base64 ç¼–ç 
        have_chart, chart_base64 = check_and_encode_chart(query_id)
        
        # 8. æ”¶é›†æ•°æ®é›†å…ƒæ•°æ®
        dataset_metadata = {
            "dataset_id": query_request.dataset_id,
            "dataset_name": dataset_info.get('name'),
            "file_path": file_path,
            "row_count": query_engine.get_data_info().get('shape', [0, 0])[0],
            "column_count": query_engine.get_data_info().get('shape', [0, 0])[1]
        }
        
        # 9. è·å–æ‰§è¡Œçš„SQLæŸ¥è¯¢å­—ç¬¦ä¸²
        executed_sqls_string = query_engine.get_executed_sqls_string()
        
        return QueryResponse(
            success=True,
            message="æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸ",
            data={
                "result": result,
                "result_type": result_type,
                "execution_time": execution_time,
                "query_id": query_id,
                "have_chart": have_chart,
                "chart_base64": chart_base64,
                "dataset_info": dataset_metadata,
                "sql_queries": executed_sqls_string
            },
            timestamp=datetime.utcnow().isoformat()
        )
        
    except HTTPException:
        raise
    except Exception as e:
        execution_time = time.time() - start_time
        error_detail = f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {str(e)}"
        
        # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
        logger.error(f"âŒ æŸ¥è¯¢æ‰§è¡Œå¤±è´¥ - ID: {query_id}")
        logger.error(f"æ•°æ®é›†: {query_request.dataset_id}")
        logger.error(f"é—®é¢˜: {query_request.question}")
        logger.error(f"é”™è¯¯: {error_detail}")
        logger.error(f"è¯¦ç»†ä¿¡æ¯: {traceback.format_exc()}")
        
        # å³ä½¿å‡ºé”™ä¹Ÿæ£€æŸ¥æ˜¯å¦æœ‰å›¾è¡¨ç”Ÿæˆ
        have_chart, chart_base64 = check_and_encode_chart(query_id)
        
        # å°è¯•è·å–SQLæŸ¥è¯¢å­—ç¬¦ä¸²ï¼ˆå¦‚æœæŸ¥è¯¢å¼•æ“å·²åˆ›å»ºï¼‰
        executed_sqls_string = None
        try:
            if 'query_engine' in locals():
                executed_sqls_string = query_engine.get_executed_sqls_string()
        except:
            pass
        
        return QueryResponse(
            success=False,
            message="æŸ¥è¯¢æ‰§è¡Œå¤±è´¥",
            data={
                "result": error_detail,
                "result_type": "error",
                "execution_time": execution_time,
                "query_id": query_id,
                "have_chart": have_chart,
                "chart_base64": chart_base64,
                "dataset_info": {"dataset_id": query_request.dataset_id},
                "sql_queries": executed_sqls_string
            },
            error=error_detail,
            timestamp=datetime.utcnow().isoformat()
        )

@router.get("/datasets", summary="è·å–å¯ç”¨æ•°æ®é›†åˆ—è¡¨")
async def list_datasets(
    tree_node_id: Optional[str] = None,
    db: SchemaDatabase = Depends(get_database)
):
    """
    è·å–æ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†åˆ—è¡¨
    
    Args:
        tree_node_id: æ ‘èŠ‚ç‚¹IDï¼Œå¦‚æœæŒ‡å®šåˆ™è¿”å›è¯¥èŠ‚ç‚¹åŠå…¶å­èŠ‚ç‚¹ä¸‹çš„æ•°æ®é›†
                     å¦‚æœä¸ä¼ æˆ–ä¼ "0"ï¼Œåˆ™è¿”å›æ‰€æœ‰æ•°æ®é›†
    """
    try:
        with LogContext(logger, f"è·å–æ•°æ®é›†åˆ—è¡¨ (tree_node_id: {tree_node_id})"):
            datasets = db.list_all_datasets(tree_node_id=tree_node_id)
        
        # è½¬æ¢ä¸ºAPIå“åº”æ ¼å¼
        dataset_list = []
        for dataset in datasets:
            dataset_list.append({
                "id": dataset.get("id") or dataset.get("path", "").split("/")[-1],
                "name": dataset.get("name", ""),
                "description": dataset.get("description", ""),
                "file_path": dataset.get("file_path", ""),
                "tree_node_id": dataset.get("tree_node_id"),
                "status": dataset.get("status", "active"),
                "created_at": dataset.get("created_at", "").isoformat() if dataset.get("created_at") else None
            })
        
        return StandardResponse(
            success=True,
            message=f"æˆåŠŸè·å–{len(dataset_list)}ä¸ªæ•°æ®é›† (tree_node_id: {tree_node_id or 'all'})",
            data={
                "datasets": dataset_list,
                "total": len(dataset_list)
            },
            timestamp=datetime.utcnow().isoformat()
        )
        
    except Exception as e:
        logger.error(f"è·å–æ•°æ®é›†åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–æ•°æ®é›†åˆ—è¡¨å¤±è´¥: {str(e)}"
        )

@router.get("/datasets/{dataset_id}", summary="è·å–æ•°æ®é›†è¯¦ç»†ä¿¡æ¯")
async def get_dataset_info(
    dataset_id: str,
    db: SchemaDatabase = Depends(get_database)
):
    """
    è·å–æŒ‡å®šæ•°æ®é›†çš„è¯¦ç»†ä¿¡æ¯
    """
    try:
        with LogContext(logger, f"è·å–æ•°æ®é›†ä¿¡æ¯: {dataset_id}"):
            dataset_info = db.get_dataset_by_id(dataset_id)
            
            if not dataset_info:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail=f"æ•°æ®é›†ä¸å­˜åœ¨: {dataset_id}"
                )
            
            # è·å–schemaé…ç½®
            schema_config = db.get_dataset_schema(dataset_id)
            
            # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œè·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯
            data_stats = {}
            # ä¼˜å…ˆä½¿ç”¨actual_data_pathï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨file_pathï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
            actual_data_path = dataset_info.get('actual_data_path')
            analysis_file_path = actual_data_path if actual_data_path else dataset_info.get('file_path')
            
            if analysis_file_path and os.path.exists(analysis_file_path):
                try:
                    from ...core.data_analyzer import DataAnalyzer
                    df = DataAnalyzer.load_data(analysis_file_path)
                    data_info = DataAnalyzer.analyze_structure(df)
                    data_stats = {
                        "row_count": data_info['shape'][0],
                        "column_count": data_info['shape'][1],
                        "columns": data_info['columns'],
                        "date_columns": data_info.get('date_columns', [])
                    }
                    
                    # æ·»åŠ æ–‡ä»¶è½¬æ¢ä¿¡æ¯
                    if dataset_info.get('is_converted'):
                        data_stats["conversion_info"] = {
                            "is_converted": True,
                            "original_type": dataset_info.get('original_file_type'),
                            "data_type": dataset_info.get('data_file_type'),
                            "original_file": dataset_info.get('file_path'),
                            "data_file": actual_data_path
                        }
                except Exception as e:
                    logger.error(f"åˆ†ææ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}")
            
            # æ„å»ºå“åº”æ•°æ®
            response_data = {
                "id": dataset_info.get("id") or dataset_id,
                "name": dataset_info.get("name", ""),
                "description": dataset_info.get("description", ""),
                "file_path": dataset_info.get("file_path", ""),
                "status": dataset_info.get("status", "active"),
                "created_at": dataset_info.get("created_at", "").isoformat() if dataset_info.get("created_at") else None,
                "updated_at": dataset_info.get("updated_at", "").isoformat() if dataset_info.get("updated_at") else None,
                "has_schema": bool(schema_config),
                "data_stats": data_stats
            }
            
            return StandardResponse(
                success=True,
                message="æˆåŠŸè·å–æ•°æ®é›†ä¿¡æ¯",
                data=response_data,
                timestamp=datetime.utcnow().isoformat()
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ•°æ®é›†ä¿¡æ¯å¤±è´¥: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–æ•°æ®é›†ä¿¡æ¯å¤±è´¥: {str(e)}"
        )

@router.get("/history", summary="è·å–æŸ¥è¯¢å†å²")
async def get_query_history(
    dataset_id: str = None,
    limit: int = 10,
    db: SchemaDatabase = Depends(get_database)
):
    """
    è·å–æŸ¥è¯¢å†å²è®°å½•
    """
    try:
        # æš‚æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼ˆæŸ¥è¯¢å†å²åŠŸèƒ½å¾…å®ç°ï¼‰
        return StandardResponse(
            success=True,
            message="æŸ¥è¯¢å†å²åŠŸèƒ½å¾…å®ç°",
            data={
                "history": [],
                "total": 0,
                "note": "æŸ¥è¯¢å†å²åŠŸèƒ½å°†åœ¨åç»­ç‰ˆæœ¬ä¸­å®ç°"
            },
            timestamp=datetime.utcnow().isoformat()
        )
        
    except Exception as e:
        logger.error(f"è·å–æŸ¥è¯¢å†å²å¤±è´¥: {str(e)}")
        return StandardResponse(
            success=False,
            message="è·å–æŸ¥è¯¢å†å²å¤±è´¥",
            error=str(e),
            timestamp=datetime.utcnow().isoformat()
        ) 