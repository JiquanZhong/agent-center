"""
æ•°æ®é›†ç®¡ç†APIè·¯ç”±

æä¾›æ•°æ®é›†çš„ä¸Šä¼ ã€é…ç½®å’ŒæŸ¥è¯¢åŠŸèƒ½
"""

from fastapi import APIRouter, Depends, HTTPException, status, UploadFile, File, Form
from typing import List, Optional, Dict, Any
import os
import uuid
import time
import json
import aiofiles
from pathlib import Path
from datetime import datetime

from ..models import (
    DatasetCreateRequest, DatasetUpdateRequest, DatasetInfo,
    StandardResponse, PaginatedResponse
)
from ..dependencies import get_database, get_settings
from ...utils.schema_database import SchemaDatabase
from ...config.settings import Settings
from ...core.data_analyzer import DataAnalyzer
from ...core.embedding_service import EmbeddingService
from ...core.vector_search_service import VectorSearchService
from ...core.intent_engine import IntentRecognitionEngine
from ...utils.logger import get_logger, LogContext
from ...utils.geo_converter import GeoConverter  # å¯¼å…¥åœ°ç†æ•°æ®è½¬æ¢å™¨

router = APIRouter(prefix="/datasets", tags=["æ•°æ®é›†ç®¡ç†"])
logger = get_logger(__name__)

# å…¨å±€å˜é‡ç”¨äºç¼“å­˜æœåŠ¡å®ä¾‹
_embedding_service = None
_vector_service = None
_intent_engine = None

def get_embedding_service(settings: Settings = Depends(get_settings)) -> EmbeddingService:
    """è·å–è¿œç¨‹embeddingæœåŠ¡å®ä¾‹"""
    global _embedding_service
    if _embedding_service is None:
        logger.info("åˆå§‹åŒ–è¿œç¨‹embeddingæœåŠ¡")
        embedding_config = settings.get_embedding_config()
        _embedding_service = EmbeddingService(
            api_key=settings.api_key,  # ä½¿ç”¨OpenAI API Keyä½œä¸ºè®¤è¯
            base_url=embedding_config["base_url"],
            model_name=embedding_config["model"]
        )
    return _embedding_service

def get_vector_service(settings: Settings = Depends(get_settings)) -> VectorSearchService:
    """è·å–ESå‘é‡æ£€ç´¢æœåŠ¡å®ä¾‹"""
    global _vector_service
    if _vector_service is None:
        logger.info("åˆå§‹åŒ–ESå‘é‡æ£€ç´¢æœåŠ¡")
        # ä»è®¾ç½®ä¸­è·å–ESé…ç½®
        es_config = settings.get_elasticsearch_config()
        _vector_service = VectorSearchService(es_config, settings.elasticsearch_index)
    return _vector_service

def get_intent_engine(
    embedding_service: EmbeddingService = Depends(get_embedding_service),
    vector_service: VectorSearchService = Depends(get_vector_service),
    db: SchemaDatabase = Depends(get_database)
) -> IntentRecognitionEngine:
    """è·å–æ„å›¾è¯†åˆ«å¼•æ“å®ä¾‹"""
    global _intent_engine
    if _intent_engine is None:
        logger.info("åˆå§‹åŒ–æ„å›¾è¯†åˆ«å¼•æ“")
        _intent_engine = IntentRecognitionEngine(
            embedding_service=embedding_service,
            vector_service=vector_service,
            db=db
        )
    return _intent_engine

async def sync_dataset_to_es(dataset_id: str, db: SchemaDatabase, 
                           intent_engine: IntentRecognitionEngine,
                           operation: str = "index") -> bool:
    """
    åŒæ­¥å•ä¸ªæ•°æ®é›†åˆ°ES
    
    Args:
        dataset_id: æ•°æ®é›†ID
        db: æ•°æ®åº“æœåŠ¡
        intent_engine: æ„å›¾è¯†åˆ«å¼•æ“
        operation: æ“ä½œç±»å‹ ("index", "update", "delete")
        
    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    try:
        if operation == "delete":
            # åˆ é™¤ESä¸­çš„æ•°æ®é›†ç´¢å¼•
            success = intent_engine.vector_service.delete_dataset(dataset_id)
            if success:
                logger.info(f"âœ… æ•°æ®é›† {dataset_id} å·²ä»ESä¸­åˆ é™¤")
            else:
                logger.warning(f"âš ï¸ æ•°æ®é›† {dataset_id} ESåˆ é™¤å¤±è´¥")
            return success
        
        # è·å–æ•°æ®é›†ä¿¡æ¯
        dataset = db.get_dataset_by_id(dataset_id)
        if not dataset:
            logger.error(f"æ•°æ®é›† {dataset_id} ä¸å­˜åœ¨ï¼Œæ— æ³•åŒæ­¥åˆ°ES")
            return False
        
        # è·å–åˆ—ä¿¡æ¯æ„å»ºå®Œæ•´çš„æ•°æ®é›†ä¿¡æ¯
        columns = db.list_dataset_columns(dataset_id)
        columns_info = ", ".join([f"{col['name']}({col['type']})" for col in columns])
        
        # æ„å»ºå®Œæ•´çš„æ•°æ®é›†ä¿¡æ¯
        dataset_info = {
            "name": dataset.get('name', ''),
            "description": dataset.get('description', ''),
            "keywords": intent_engine._generate_keywords_from_dataset(dataset, columns),
            "domain": intent_engine._infer_domain_from_dataset(dataset, columns),
            "data_summary": intent_engine._generate_data_summary(dataset, columns),
            "columns_info": columns_info,
            "tree_node_id": dataset.get('tree_node_id', ''),
            "file_path": dataset.get('actual_data_path') or dataset.get('file_path', ''),
            "status": dataset.get('status', 'active'),
            "created_at": dataset.get('created_at'),
            "updated_at": dataset.get('updated_at')
        }
        
        # ç”Ÿæˆå‘é‡
        embedding = intent_engine.embedding_service.generate_dataset_embedding(dataset_info)
        
        # æ ¹æ®æ“ä½œç±»å‹æ‰§è¡Œç›¸åº”çš„ESæ“ä½œ
        if operation == "index":
            success = intent_engine.vector_service.index_dataset(dataset_id, dataset_info, embedding)
        elif operation == "update":
            success = intent_engine.vector_service.update_dataset(dataset_id, dataset_info, embedding)
        else:
            success = intent_engine.vector_service.index_dataset(dataset_id, dataset_info, embedding)
        
        if success:
            logger.info(f"âœ… æ•°æ®é›† {dataset_id} ({dataset.get('name')}) å·²åŒæ­¥åˆ°ES ({operation})")
        else:
            logger.error(f"âŒ æ•°æ®é›† {dataset_id} åŒæ­¥åˆ°ESå¤±è´¥ ({operation})")
        
        return success
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®é›† {dataset_id} åŒæ­¥åˆ°ESå¼‚å¸¸: {e}")
        return False

def sanitize_filename(filename: str) -> str:
    """
    æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤å±é™©å­—ç¬¦ï¼Œä¿æŒåŸå§‹åç§°çš„å¯è¯»æ€§
    
    Args:
        filename: åŸå§‹æ–‡ä»¶å
        
    Returns:
        str: æ¸…ç†åçš„å®‰å…¨æ–‡ä»¶å
    """
    import re
    
    # ç§»é™¤è·¯å¾„åˆ†éš”ç¬¦å’Œå…¶ä»–å±é™©å­—ç¬¦
    dangerous_chars = r'[<>:"/\\|?*\x00-\x1f]'
    safe_filename = re.sub(dangerous_chars, '_', filename)
    
    # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œç‚¹
    safe_filename = re.sub(r'\.+$', '', safe_filename.strip())
    
    # ç¡®ä¿æ–‡ä»¶åä¸ä¸ºç©º
    if not safe_filename:
        safe_filename = "unnamed_file"
    
    # é™åˆ¶æ–‡ä»¶åé•¿åº¦ï¼ˆWindowsé™åˆ¶ä¸º255å­—ç¬¦ï¼‰
    if len(safe_filename) > 200:
        name_part = safe_filename[:190]
        ext_part = safe_filename[190:]
        safe_filename = name_part + ext_part
    
    return safe_filename

@router.post("/upload", summary="ä¸Šä¼ æ•°æ®æ–‡ä»¶")
async def upload_dataset(
    file: UploadFile = File(..., description="æ•°æ®æ–‡ä»¶(CSV/Excel)"),
    name: str = Form(..., description="æ•°æ®é›†åç§°"),
    description: Optional[str] = Form(None, description="æ•°æ®é›†æè¿°"),
    tree_node_id: Optional[str] = Form(None, description="æ‰€å±èŠ‚ç‚¹ID"),
    auto_analyze: bool = Form(True, description="æ˜¯å¦è‡ªåŠ¨åˆ†æç”Ÿæˆè¯­ä¹‰é…ç½®"),
    db: SchemaDatabase = Depends(get_database),
    intent_engine: IntentRecognitionEngine = Depends(get_intent_engine)
):
    """
    ä¸Šä¼ æ•°æ®æ–‡ä»¶å¹¶åˆ›å»ºæ•°æ®é›†
    
    æµç¨‹ï¼š
    1. éªŒè¯æ–‡ä»¶æ ¼å¼
    2. éªŒè¯æ ‘èŠ‚ç‚¹ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    3. ä¿å­˜æ–‡ä»¶
    4. å¦‚æœauto_analyzeä¸ºTrueï¼Œè‡ªåŠ¨åˆ†ææ•°æ®ç»“æ„å¹¶ç”Ÿæˆè¯­ä¹‰é…ç½®
    5. åˆ›å»ºæ•°æ®é›†è®°å½•
    """
    
    try:
        with LogContext(logger, f"ä¸Šä¼ æ•°æ®æ–‡ä»¶: {file.filename}"):
            # 1. éªŒè¯æ–‡ä»¶æ ¼å¼
            allowed_extensions = ['.csv', '.shp', '.zip']
            file_ext = Path(file.filename).suffix.lower()
            
            if file_ext not in allowed_extensions:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}ã€‚æ”¯æŒæ ¼å¼: {', '.join(allowed_extensions)}\n\nğŸ’¡ æç¤ºï¼š\n- CSVæ–‡ä»¶ï¼šç›´æ¥ä¸Šä¼ \n- SHPæ–‡ä»¶ï¼šéœ€è¦ä¸Šä¼ åŒ…å«.shpã€.shxã€.dbfç­‰å®Œæ•´æ–‡ä»¶çš„ZIPå‹ç¼©åŒ…\n- å•ç‹¬çš„.shpæ–‡ä»¶æ— æ³•ä½¿ç”¨ï¼Œè¯·æ‰“åŒ…åä¸Šä¼ "
                )
            
            # 2. éªŒè¯æ ‘èŠ‚ç‚¹ï¼ˆå¦‚æœæŒ‡å®šï¼‰
            if tree_node_id and tree_node_id != "0":
                existing_node = db.get_tree_node(tree_node_id)
                if not existing_node:
                    raise HTTPException(
                        status_code=status.HTTP_400_BAD_REQUEST,
                        detail=f"æŒ‡å®šçš„æ ‘èŠ‚ç‚¹ä¸å­˜åœ¨: {tree_node_id}"
                    )
                logger.info(f"æ•°æ®é›†å°†å½’å±åˆ°èŠ‚ç‚¹: {existing_node['name']} (ID: {tree_node_id})")
            
            # 2. ç”Ÿæˆä¸´æ—¶ç›®å½•ï¼ˆå…ˆåˆ›å»ºæ•°æ®é›†è®°å½•è·å–è‡ªå¢IDï¼‰
            temp_id = uuid.uuid4()  # ä¸´æ—¶ç”¨äºç›®å½•åˆ›å»º
            upload_dir = Path("datasets") / str(temp_id)
            upload_dir.mkdir(parents=True, exist_ok=True)
            
            # 3. ä¿å­˜æ–‡ä»¶ï¼ˆä¿ç•™åŸå§‹æ–‡ä»¶åï¼‰
            timestamp = int(time.time())
            safe_filename = sanitize_filename(file.filename)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™æ·»åŠ æ—¶é—´æˆ³é¿å…å†²çª
            file_path = upload_dir / safe_filename
            if file_path.exists():
                name_part = Path(safe_filename).stem
                ext_part = Path(safe_filename).suffix
                safe_filename = f"{name_part}_{timestamp}{ext_part}"
                file_path = upload_dir / safe_filename
            
            logger.info(f"ä¿å­˜æ–‡ä»¶åˆ°: {file_path}")
            
            async with aiofiles.open(file_path, 'wb') as f:
                content = await file.read()
                await f.write(content)
            
            # 3.5. å¤„ç†ZIPæ–‡ä»¶ï¼ˆå¯èƒ½åŒ…å«SHPæ–‡ä»¶ï¼‰
            if file_ext == '.zip':
                logger.info("æ£€æµ‹åˆ°ZIPæ–‡ä»¶ï¼Œå¼€å§‹è§£å‹")
                try:
                    import zipfile
                    
                    # è§£å‹ZIPæ–‡ä»¶ï¼Œå¤„ç†ä¸­æ–‡ç¼–ç é—®é¢˜
                    with zipfile.ZipFile(file_path, 'r') as zip_ref:
                        # æ˜¾ç¤ºZIPæ–‡ä»¶å†…å®¹ç”¨äºè°ƒè¯•
                        file_list = zip_ref.namelist()
                        logger.info(f"ZIPæ–‡ä»¶åŒ…å« {len(file_list)} ä¸ªæ–‡ä»¶:")
                        for file_name in file_list[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªæ–‡ä»¶
                            logger.info(f"  - {file_name}")
                        if len(file_list) > 10:
                            logger.info(f"  ... è¿˜æœ‰ {len(file_list) - 10} ä¸ªæ–‡ä»¶")
                        
                        # å¤„ç†ä¸­æ–‡ç¼–ç é—®é¢˜
                        for member in zip_ref.infolist():
                            # å°è¯•ç”¨GBKç¼–ç è§£ç æ–‡ä»¶åï¼ˆå¤„ç†ä¸­æ–‡ä¹±ç ï¼‰
                            try:
                                # å…ˆå°è¯•UTF-8è§£ç 
                                member.filename = member.filename.encode('cp437').decode('utf-8')
                            except (UnicodeDecodeError, UnicodeEncodeError):
                                try:
                                    # å¦‚æœUTF-8å¤±è´¥ï¼Œå°è¯•GBKè§£ç 
                                    member.filename = member.filename.encode('cp437').decode('gbk')
                                    logger.debug(f"ä½¿ç”¨GBKç¼–ç è§£ç æ–‡ä»¶å: {member.filename}")
                                except (UnicodeDecodeError, UnicodeEncodeError):
                                    # å¦‚æœéƒ½å¤±è´¥ï¼Œä¿æŒåŸå§‹æ–‡ä»¶å
                                    logger.warning(f"æ— æ³•è§£ç æ–‡ä»¶åï¼Œä¿æŒåŸå§‹: {member.filename}")
                            
                            # è§£å‹å•ä¸ªæ–‡ä»¶
                            zip_ref.extract(member, upload_dir)
                    
                    # é€’å½’æŸ¥æ‰¾SHPæ–‡ä»¶ï¼ˆæ”¯æŒå­ç›®å½•ï¼‰
                    shp_files = list(upload_dir.rglob("*.shp"))
                    logger.info(f"åœ¨è§£å‹ç›®å½•ä¸­æ‰¾åˆ° {len(shp_files)} ä¸ªSHPæ–‡ä»¶")
                    
                    if not shp_files:
                        # å°è¯•æŸ¥æ‰¾ä¸åŒå¤§å°å†™çš„æ‰©å±•å
                        shp_files_upper = list(upload_dir.rglob("*.SHP"))
                        if shp_files_upper:
                            shp_files = shp_files_upper
                            logger.info(f"æ‰¾åˆ°å¤§å†™æ‰©å±•åçš„SHPæ–‡ä»¶: {len(shp_files)} ä¸ª")
                        else:
                            # åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶ç”¨äºè°ƒè¯•
                            all_files = list(upload_dir.rglob("*"))
                            logger.error(f"æœªæ‰¾åˆ°SHPæ–‡ä»¶ï¼Œè§£å‹åçš„æ‰€æœ‰æ–‡ä»¶:")
                            for f in all_files:
                                if f.is_file():
                                    logger.error(f"  - {f.relative_to(upload_dir)} (æ‰©å±•å: {f.suffix})")
                            
                            raise HTTPException(
                                status_code=status.HTTP_400_BAD_REQUEST,
                                detail="ZIPæ–‡ä»¶ä¸­æœªæ‰¾åˆ°SHPæ–‡ä»¶\n\nğŸ’¡ è¯·ç¡®ä¿ï¼š\n1. ZIPæ–‡ä»¶ä¸­åŒ…å«.shpæ–‡ä»¶\n2. æ–‡ä»¶æ‰©å±•åæ­£ç¡®(.shpæˆ–.SHP)\n3. æ–‡ä»¶æ²¡æœ‰æŸå"
                            )
                    
                    if len(shp_files) > 1:
                        logger.warning(f"ZIPæ–‡ä»¶ä¸­åŒ…å«å¤šä¸ªSHPæ–‡ä»¶: {[f.name for f in shp_files]}")
                        # é€‰æ‹©ç¬¬ä¸€ä¸ªSHPæ–‡ä»¶ï¼Œä½†ç»™å‡ºè­¦å‘Š
                        shp_file_path = shp_files[0]
                        logger.info(f"é€‰æ‹©ç¬¬ä¸€ä¸ªSHPæ–‡ä»¶: {shp_file_path.name}")
                    else:
                        shp_file_path = shp_files[0]
                    
                    logger.info(f"é€‰ä¸­çš„SHPæ–‡ä»¶: {shp_file_path}")
                    
                    # éªŒè¯å¿…è¦çš„é™„å±æ–‡ä»¶ï¼ˆåœ¨åŒä¸€ç›®å½•ä¸­æŸ¥æ‰¾ï¼‰
                    shp_dir = shp_file_path.parent
                    base_name = shp_file_path.stem
                    required_files = ['.shx', '.dbf']
                    missing_files = []
                    found_files = []
                    
                    for ext in required_files:
                        # å°è¯•ä¸åŒå¤§å°å†™
                        required_file_lower = shp_dir / f"{base_name}{ext}"
                        required_file_upper = shp_dir / f"{base_name}{ext.upper()}"
                        
                        if required_file_lower.exists():
                            found_files.append(ext)
                        elif required_file_upper.exists():
                            found_files.append(ext.upper())
                        else:
                            missing_files.append(ext)
                    
                    logger.info(f"æ‰¾åˆ°çš„é™„å±æ–‡ä»¶: {found_files}")
                    
                    if missing_files:
                        # åˆ—å‡ºSHPæ–‡ä»¶åŒç›®å½•çš„æ‰€æœ‰æ–‡ä»¶
                        same_dir_files = list(shp_dir.glob(f"{base_name}.*"))
                        logger.error(f"SHPæ–‡ä»¶ '{base_name}' åŒç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶:")
                        for f in same_dir_files:
                            logger.error(f"  - {f.name}")
                        
                        raise HTTPException(
                            status_code=status.HTTP_400_BAD_REQUEST,
                            detail=f"ZIPæ–‡ä»¶ä¸­ç¼ºå°‘å¿…è¦çš„Shapefileé™„å±æ–‡ä»¶: {', '.join(missing_files)}\n\nğŸ’¡ å®Œæ•´çš„Shapefileéœ€è¦åŒ…å«:\n- {base_name}.shp (ä¸»æ–‡ä»¶)\n- {base_name}.shx (ç´¢å¼•æ–‡ä»¶)\n- {base_name}.dbf (å±æ€§æ–‡ä»¶)\n\nå½“å‰åªæ‰¾åˆ°: {', '.join(found_files)}"
                        )
                    
                    # æ›´æ–°file_pathä¸ºSHPæ–‡ä»¶è·¯å¾„ï¼Œå¹¶æ ‡è®°ä¸ºä»ZIPè§£å‹
                    file_path = shp_file_path
                    file_ext = '.shp'  # åç»­æŒ‰SHPæ–‡ä»¶å¤„ç†
                    logger.info(f"ZIPæ–‡ä»¶è§£å‹æˆåŠŸï¼Œå°†æŒ‰SHPæ–‡ä»¶å¤„ç†: {file_path}")
                    
                except zipfile.BadZipFile:
                    raise HTTPException(
                        status_code=status.HTTP_400_BAD_REQUEST,
                        detail="æ— æ•ˆçš„ZIPæ–‡ä»¶æ ¼å¼"
                    )
                except Exception as e:
                    logger.error(f"ZIPæ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}")
                    raise HTTPException(
                        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                        detail=f"ZIPæ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}"
                    )

            # 3.6. å¤„ç†SHPæ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
            converted_csv_path = None
            actual_data_path = str(file_path)  # ç”¨äºåç»­åˆ†æçš„å®é™…æ•°æ®æ–‡ä»¶è·¯å¾„
            conversion_info = {}  # åˆå§‹åŒ–è½¬æ¢ä¿¡æ¯
            
            if file_ext == '.shp':
                logger.info("æ£€æµ‹åˆ°SHPæ–‡ä»¶ï¼Œå¼€å§‹è½¬æ¢ä¸ºCSV")
                try:
                    # éªŒè¯SHPæ–‡ä»¶
                    if not GeoConverter.validate_shp_file(str(file_path)):
                        raise HTTPException(
                            status_code=status.HTTP_400_BAD_REQUEST,
                            detail="SHPæ–‡ä»¶æ— æ•ˆæˆ–ç¼ºå°‘å¿…è¦çš„é™„å±æ–‡ä»¶ï¼ˆ.shx, .dbfï¼‰"
                        )
                    
                    # è·å–SHPæ–‡ä»¶ä¿¡æ¯
                    shp_info = GeoConverter.get_shp_info(str(file_path))
                    logger.info(f"SHPæ–‡ä»¶ä¿¡æ¯: {shp_info}")
                    
                    # è½¬æ¢ä¸ºCSVï¼ˆåŸºäºåŸå§‹æ–‡ä»¶åï¼‰
                    csv_filename = f"{Path(safe_filename).stem}.csv"
                    converted_csv_path = upload_dir / csv_filename
                    
                    # è½¬æ¢SHPåˆ°CSV
                    GeoConverter.shp_to_csv(str(file_path), str(converted_csv_path))
                    logger.info(f"SHPæ–‡ä»¶å·²è½¬æ¢ä¸ºCSV: {converted_csv_path}")
                    
                    # æ›´æ–°å®é™…æ•°æ®æ–‡ä»¶è·¯å¾„
                    actual_data_path = str(converted_csv_path)
                    
                    # è®¾ç½®è½¬æ¢ä¿¡æ¯
                    conversion_info = {
                        "shp_info": shp_info,
                        "to_format": "csv",
                        "from_format": "shp",
                        "original_file": str(file_path),
                        "converted_file": str(converted_csv_path),
                        "conversion_time": datetime.utcnow().timestamp()
                    }
                    
                except Exception as e:
                    logger.error(f"SHPæ–‡ä»¶è½¬æ¢å¤±è´¥: {str(e)}")
                    raise HTTPException(
                        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                        detail=f"SHPæ–‡ä»¶è½¬æ¢å¤±è´¥: {str(e)}"
                    )
            
            # 4. å…ˆè·å–æ•°æ®é›†IDï¼ˆä½¿ç”¨ä¸´æ—¶æ•°æ®åˆ›å»ºï¼‰
            temp_dataset_data = {
                'name': name,
                'description': description,
                'file_path': 'temp',  # ä¸´æ—¶è·¯å¾„ï¼Œç¨åæ›´æ–°
                'original_filename': file.filename,
                'original_file_type': file_ext.lstrip('.'),
                'actual_data_path': 'temp',  # ä¸´æ—¶è·¯å¾„ï¼Œç¨åæ›´æ–°
                'data_file_type': 'csv',
                'is_converted': file_ext != '.csv',
                'conversion_info': json.dumps(conversion_info) if conversion_info else json.dumps({}),
                'tree_node_id': tree_node_id,  # æ·»åŠ æ ‘èŠ‚ç‚¹ID
                'status': 'active',
                'created_at': datetime.utcnow().timestamp(),
                'updated_at': datetime.utcnow().timestamp()
            }
            
            # ä½¿ç”¨ç°æœ‰çš„save_datasetæ–¹æ³•è·å–è‡ªå¢ID
            dataset_id = db.save_dataset(temp_dataset_data)
            logger.info(f"è·å–åˆ°æ•°æ®é›†ID: {dataset_id}")
            
            # 5. é‡å‘½åç›®å½•ä¸ºå®é™…IDï¼Œå¹¶æ›´æ–°æ‰€æœ‰è·¯å¾„
            final_upload_dir = Path("datasets") / str(dataset_id)
            final_file_path = None
            final_actual_data_path = None
            
            try:
                # åœ¨Windowsä¸Šï¼Œå…ˆç¡®ä¿æ²¡æœ‰æ–‡ä»¶å¥æŸ„å ç”¨
                import shutil
                
                # å¦‚æœç›®æ ‡ç›®å½•å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
                if final_upload_dir.exists():
                    shutil.rmtree(final_upload_dir)
                
                # ä½¿ç”¨shutil.moveä»£æ›¿renameï¼Œæ›´å¯é 
                shutil.move(str(upload_dir), str(final_upload_dir))
                logger.info(f"é‡å‘½åç›®å½•æˆåŠŸ: {upload_dir} -> {final_upload_dir}")
                
                # è®¡ç®—æœ€ç»ˆè·¯å¾„ï¼ˆä½¿ç”¨å­—ç¬¦ä¸²æ›¿æ¢ä»£æ›¿relative_toï¼Œæ›´ç¨³å®šï¼‰
                old_dir_str = str(upload_dir).replace('\\', '/')
                new_dir_str = str(final_upload_dir).replace('\\', '/')
                
                final_file_path = str(file_path).replace('\\', '/').replace(old_dir_str, new_dir_str)
                final_actual_data_path = str(actual_data_path).replace('\\', '/').replace(old_dir_str, new_dir_str)
                
                logger.info(f"æœ€ç»ˆæ–‡ä»¶è·¯å¾„: {final_file_path}")
                logger.info(f"æœ€ç»ˆæ•°æ®è·¯å¾„: {final_actual_data_path}")
                
            except Exception as e:
                logger.warning(f"é‡å‘½åç›®å½•å¤±è´¥ï¼Œä¿æŒåŸç›®å½•: {e}")
                # å¦‚æœé‡å‘½åå¤±è´¥ï¼Œä½¿ç”¨åŸè·¯å¾„
                final_file_path = str(file_path)
                final_actual_data_path = str(actual_data_path)
            
            # 6. æ›´æ–°æ•°æ®åº“ä¸­çš„æ­£ç¡®è·¯å¾„
            try:
                db.update_dataset(dataset_id, {
                    'file_path': final_file_path,
                    'actual_data_path': final_actual_data_path
                })
                logger.info(f"æ•°æ®åº“è·¯å¾„æ›´æ–°æˆåŠŸ")
                logger.info(f"æ›´æ–°çš„file_path: {final_file_path}")
                logger.info(f"æ›´æ–°çš„actual_data_path: {final_actual_data_path}")
            except Exception as update_e:
                logger.error(f"æ›´æ–°æ•°æ®åº“è·¯å¾„å¤±è´¥: {update_e}")
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail=f"æ›´æ–°æ•°æ®åº“è·¯å¾„å¤±è´¥: {update_e}"
                )
            
            # 7. å¦‚æœå¯ç”¨äº†è‡ªåŠ¨åˆ†æï¼Œç”Ÿæˆè¯­ä¹‰é…ç½®
            if auto_analyze:
                try:
                    # è¯»å–æ•°æ®ç”¨äºåˆ†æï¼ˆä½¿ç”¨æœ€ç»ˆçš„æ•°æ®æ–‡ä»¶è·¯å¾„ï¼‰
                    df = DataAnalyzer.load_data(final_actual_data_path)
                    
                    # åˆ›å»ºåŸºç¡€è¯­ä¹‰é…ç½®
                    schema = DataAnalyzer.create_semantic_schema(
                        file_path=final_actual_data_path,
                        organization="ask-data-ai",
                        dataset_name=name.lower().replace(' ', '-')
                    )
                    
                    # è‡ªåŠ¨ç”Ÿæˆtransformationsé…ç½®
                    transformations = DataAnalyzer.auto_generate_transformations(df)
                    if transformations:
                        schema['transformations'] = transformations
                        logger.info(f"è‡ªåŠ¨ç”Ÿæˆäº† {len(transformations)} ä¸ªtransformationé…ç½®")
                    
                    # ä¿å­˜è¯­ä¹‰é…ç½®
                    db.save_schema(dataset_id, schema)
                    logger.info("è‡ªåŠ¨ç”Ÿæˆè¯­ä¹‰é…ç½®æˆåŠŸ")
                    
                except Exception as e:
                    logger.error(f"è‡ªåŠ¨ç”Ÿæˆè¯­ä¹‰é…ç½®å¤±è´¥: {str(e)}")
                    # ä¸å½±å“æ•°æ®é›†åˆ›å»ºï¼Œåªè®°å½•é”™è¯¯
            
            # 8. åŒæ­¥æ•°æ®é›†åˆ°ESå‘é‡å­˜å‚¨
            try:
                logger.info(f"ğŸ”„ å¼€å§‹åŒæ­¥æ•°æ®é›† {dataset_id} åˆ°ES")
                sync_success = await sync_dataset_to_es(
                    dataset_id=str(dataset_id), 
                    db=db, 
                    intent_engine=intent_engine,
                    operation="index"
                )
                if sync_success:
                    logger.info(f"âœ… æ•°æ®é›† {dataset_id} ESåŒæ­¥æˆåŠŸ")
                else:
                    logger.warning(f"âš ï¸ æ•°æ®é›† {dataset_id} ESåŒæ­¥å¤±è´¥")
            except Exception as sync_e:
                logger.error(f"âŒ æ•°æ®é›† {dataset_id} ESåŒæ­¥å¼‚å¸¸: {sync_e}")
                # ESåŒæ­¥å¤±è´¥ä¸å½±å“æ•°æ®é›†åˆ›å»º
            
            return StandardResponse(
                success=True,
                message="æ•°æ®é›†åˆ›å»ºæˆåŠŸ",
                data={
                    'id': dataset_id,
                    'name': name,
                    'description': description,
                    'file_path': final_file_path,
                    'actual_data_path': final_actual_data_path,
                    'original_filename': file.filename,
                    'tree_node_id': tree_node_id,
                    'is_converted': file_ext != '.csv',
                    'status': 'active',
                    'created_at': datetime.utcnow().isoformat()
                },
                timestamp=datetime.utcnow().isoformat()
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ•°æ®é›†åˆ›å»ºå¤±è´¥: {str(e)}")
        return StandardResponse(
            success=False,
            message="æ•°æ®é›†åˆ›å»ºå¤±è´¥",
            error=str(e),
            timestamp=datetime.utcnow().isoformat()
        )

@router.get("/", response_model=PaginatedResponse, summary="è·å–æ•°æ®é›†åˆ—è¡¨")
async def list_datasets(
    page: int = 1,
    per_page: int = 20,
    tree_node_id: Optional[str] = None,
    db: SchemaDatabase = Depends(get_database)
):
    """
    è·å–æ•°æ®é›†åˆ—è¡¨
    
    Args:
        page: é¡µç ï¼Œä»1å¼€å§‹
        per_page: æ¯é¡µæ•°é‡
        tree_node_id: æ ‘èŠ‚ç‚¹IDï¼Œå¦‚æœæŒ‡å®šåˆ™è¿”å›è¯¥èŠ‚ç‚¹åŠå…¶å­èŠ‚ç‚¹ä¸‹çš„æ•°æ®é›†
                     å¦‚æœä¸ä¼ æˆ–ä¼ "0"ï¼Œåˆ™è¿”å›æ‰€æœ‰æ•°æ®é›†
    """
    try:
        # è°ƒç”¨å¸¦tree_node_idå‚æ•°çš„list_datasetsæ–¹æ³•
        all_datasets = db.list_datasets(tree_node_id=tree_node_id)
        
        # æ‰‹åŠ¨å®ç°åˆ†é¡µ
        total = len(all_datasets)
        start_idx = (page - 1) * per_page
        end_idx = start_idx + per_page
        datasets = all_datasets[start_idx:end_idx]
        
        return PaginatedResponse(
            success=True,
            message=f"è·å–æ•°æ®é›†åˆ—è¡¨æˆåŠŸ (tree_node_id: {tree_node_id or 'all'})",
            data=datasets,
            pagination={
                "total": total,
                "page": page,
                "per_page": per_page,
                "pages": (total + per_page - 1) // per_page
            },
            timestamp=datetime.utcnow().isoformat()
        )
    except Exception as e:
        logger.error(f"è·å–æ•°æ®é›†åˆ—è¡¨å¤±è´¥: {str(e)}")
        # é”™è¯¯æ—¶ä¹Ÿè¦è¿”å›PaginatedResponseæ ¼å¼
        return PaginatedResponse(
            success=False,
            message="è·å–æ•°æ®é›†åˆ—è¡¨å¤±è´¥",
            data=[],
            pagination={
                "total": 0,
                "page": page,
                "per_page": per_page,
                "pages": 0
            },
            error=str(e),
            timestamp=datetime.utcnow().isoformat()
        )

@router.get("/{dataset_id}", response_model=StandardResponse, summary="è·å–æ•°æ®é›†è¯¦ç»†ä¿¡æ¯")
async def get_dataset(
    dataset_id: int,
    db: SchemaDatabase = Depends(get_database)
):
    """è·å–æ•°æ®é›†çš„è¯¦ç»†ä¿¡æ¯"""
    try:
        dataset = db.get_dataset_by_id(dataset_id)
        if not dataset:
            return StandardResponse(
                success=False,
                message="æ•°æ®é›†ä¸å­˜åœ¨",
                error=f"æ•°æ®é›† {dataset_id} ä¸å­˜åœ¨",
                timestamp=datetime.utcnow().isoformat()
            )
        
        return StandardResponse(
            success=True,
            message="è·å–æ•°æ®é›†æˆåŠŸ",
            data=dataset,
            timestamp=datetime.utcnow().isoformat()
        )
    except Exception as e:
        logger.error(f"è·å–æ•°æ®é›†å¤±è´¥: {str(e)}")
        return StandardResponse(
            success=False,
            message="è·å–æ•°æ®é›†å¤±è´¥",
            error=str(e),
            timestamp=datetime.utcnow().isoformat()
        )

@router.put("/{dataset_id}", response_model=StandardResponse, summary="æ›´æ–°æ•°æ®é›†ä¿¡æ¯")
async def update_dataset(
    dataset_id: int,
    dataset: DatasetUpdateRequest,
    db: SchemaDatabase = Depends(get_database),
    intent_engine: IntentRecognitionEngine = Depends(get_intent_engine)
):
    """æ›´æ–°æ•°æ®é›†çš„ä¿¡æ¯"""
    try:
        # éªŒè¯æ•°æ®é›†æ˜¯å¦å­˜åœ¨
        existing_dataset = db.get_dataset_by_id(dataset_id)
        if not existing_dataset:
            return StandardResponse(
                success=False,
                message="æ•°æ®é›†ä¸å­˜åœ¨",
                error=f"æ•°æ®é›† {dataset_id} ä¸å­˜åœ¨",
                timestamp=datetime.utcnow().isoformat()
            )
        
        # æ„å»ºæ›´æ–°æ•°æ®
        update_data = {}
        if dataset.name is not None:
            update_data['name'] = dataset.name
        if dataset.description is not None:
            update_data['description'] = dataset.description
        if dataset.file_path is not None:
            update_data['file_path'] = dataset.file_path
        if dataset.status is not None:
            update_data['status'] = dataset.status
        
        # ä½¿ç”¨ç°æœ‰çš„update_datasetæ–¹æ³•
        success = db.update_dataset(dataset_id, update_data)
        if success:
            # åŒæ­¥æ›´æ–°åˆ°ES
            try:
                logger.info(f"ğŸ”„ å¼€å§‹åŒæ­¥æ›´æ–°æ•°æ®é›† {dataset_id} åˆ°ES")
                sync_success = await sync_dataset_to_es(
                    dataset_id=str(dataset_id), 
                    db=db, 
                    intent_engine=intent_engine,
                    operation="update"
                )
                if sync_success:
                    logger.info(f"âœ… æ•°æ®é›† {dataset_id} ESæ›´æ–°åŒæ­¥æˆåŠŸ")
                else:
                    logger.warning(f"âš ï¸ æ•°æ®é›† {dataset_id} ESæ›´æ–°åŒæ­¥å¤±è´¥")
            except Exception as sync_e:
                logger.error(f"âŒ æ•°æ®é›† {dataset_id} ESæ›´æ–°åŒæ­¥å¼‚å¸¸: {sync_e}")
                # ESåŒæ­¥å¤±è´¥ä¸å½±å“æ•°æ®é›†æ›´æ–°
            
            # è·å–æ›´æ–°åçš„æ•°æ®é›†ä¿¡æ¯
            updated_dataset = db.get_dataset_by_id(dataset_id)
            return StandardResponse(
                success=True,
                message="æ•°æ®é›†æ›´æ–°æˆåŠŸ",
                data=updated_dataset,
                timestamp=datetime.utcnow().isoformat()
            )
        else:
            return StandardResponse(
                success=False,
                message="æ•°æ®é›†æ›´æ–°å¤±è´¥",
                error="æ›´æ–°æ“ä½œå¤±è´¥",
                timestamp=datetime.utcnow().isoformat()
            )
    except Exception as e:
        logger.error(f"æ›´æ–°æ•°æ®é›†å¤±è´¥: {str(e)}")
        return StandardResponse(
            success=False,
            message="æ›´æ–°æ•°æ®é›†å¤±è´¥",
            error=str(e),
            timestamp=datetime.utcnow().isoformat()
        )

@router.delete("/{dataset_id}", response_model=StandardResponse, summary="åˆ é™¤æ•°æ®é›†")
async def delete_dataset(
    dataset_id: int,
    delete_file: bool = True,
    db: SchemaDatabase = Depends(get_database),
    intent_engine: IntentRecognitionEngine = Depends(get_intent_engine)
):
    """åˆ é™¤æ•°æ®é›†"""
    try:
        # éªŒè¯æ•°æ®é›†æ˜¯å¦å­˜åœ¨
        dataset = db.get_dataset_by_id(dataset_id)
        if not dataset:
            return StandardResponse(
                success=False,
                message="æ•°æ®é›†ä¸å­˜åœ¨",
                error=f"æ•°æ®é›† {dataset_id} ä¸å­˜åœ¨",
                timestamp=datetime.utcnow().isoformat()
            )
        
        # ä½¿ç”¨ç°æœ‰çš„delete_dataset_by_idæ–¹æ³•
        success = db.delete_dataset_by_id(dataset_id)
        if success:
            # åŒæ­¥åˆ é™¤ESä¸­çš„ç´¢å¼•
            try:
                logger.info(f"ğŸ”„ å¼€å§‹ä»ESä¸­åˆ é™¤æ•°æ®é›† {dataset_id}")
                sync_success = await sync_dataset_to_es(
                    dataset_id=str(dataset_id), 
                    db=db, 
                    intent_engine=intent_engine,
                    operation="delete"
                )
                if sync_success:
                    logger.info(f"âœ… æ•°æ®é›† {dataset_id} ESåˆ é™¤åŒæ­¥æˆåŠŸ")
                else:
                    logger.warning(f"âš ï¸ æ•°æ®é›† {dataset_id} ESåˆ é™¤åŒæ­¥å¤±è´¥")
            except Exception as sync_e:
                logger.error(f"âŒ æ•°æ®é›† {dataset_id} ESåˆ é™¤åŒæ­¥å¼‚å¸¸: {sync_e}")
                # ESåŒæ­¥å¤±è´¥ä¸å½±å“æ•°æ®é›†åˆ é™¤
            
            return StandardResponse(
                success=True,
                message="æ•°æ®é›†åˆ é™¤æˆåŠŸ",
                timestamp=datetime.utcnow().isoformat()
            )
        else:
            return StandardResponse(
                success=False,
                message="æ•°æ®é›†åˆ é™¤å¤±è´¥",
                error="åˆ é™¤æ“ä½œå¤±è´¥",
                timestamp=datetime.utcnow().isoformat()
            )
    except Exception as e:
        logger.error(f"åˆ é™¤æ•°æ®é›†å¤±è´¥: {str(e)}")
        return StandardResponse(
            success=False,
            message="åˆ é™¤æ•°æ®é›†å¤±è´¥",
            error=str(e),
            timestamp=datetime.utcnow().isoformat()
        )

@router.post("/sync-to-es", response_model=StandardResponse, summary="æ‰‹åŠ¨åŒæ­¥æ•°æ®é›†åˆ°ES")
async def sync_datasets_to_es(
    force_refresh: bool = False,
    dataset_ids: Optional[List[int]] = None,
    db: SchemaDatabase = Depends(get_database),
    intent_engine: IntentRecognitionEngine = Depends(get_intent_engine)
):
    """
    æ‰‹åŠ¨åŒæ­¥æ•°æ®é›†åˆ°ESå‘é‡å­˜å‚¨
    
    Args:
        force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°æ‰€æœ‰æ•°æ®
        dataset_ids: æŒ‡å®šè¦åŒæ­¥çš„æ•°æ®é›†IDåˆ—è¡¨ï¼Œå¦‚æœä¸ºç©ºåˆ™åŒæ­¥æ‰€æœ‰æ•°æ®é›†
    """
    try:
        with LogContext(logger, "æ‰‹åŠ¨åŒæ­¥æ•°æ®é›†åˆ°ES"):
            # å¦‚æœæŒ‡å®šäº†ç‰¹å®šçš„æ•°æ®é›†IDï¼Œåˆ™åªåŒæ­¥è¿™äº›æ•°æ®é›†
            if dataset_ids:
                success_count = 0
                failed_count = 0
                errors = []
                
                for dataset_id in dataset_ids:
                    try:
                        sync_success = await sync_dataset_to_es(
                            dataset_id=str(dataset_id), 
                            db=db, 
                            intent_engine=intent_engine,
                            operation="index"
                        )
                        if sync_success:
                            success_count += 1
                        else:
                            failed_count += 1
                            errors.append(f"æ•°æ®é›† {dataset_id} åŒæ­¥å¤±è´¥")
                    except Exception as e:
                        failed_count += 1
                        errors.append(f"æ•°æ®é›† {dataset_id} åŒæ­¥å¼‚å¸¸: {str(e)}")
                
                sync_result = {
                    "total_count": len(dataset_ids),
                    "success_count": success_count,
                    "failed_count": failed_count,
                    "errors": errors
                }
            else:
                # ä½¿ç”¨æ„å›¾å¼•æ“çš„æ‰¹é‡åŒæ­¥åŠŸèƒ½
                sync_result = intent_engine.sync_datasets_to_vector_store(force_refresh)
            
            return StandardResponse(
                success=True,
                message=f"æ•°æ®é›†ESåŒæ­¥å®Œæˆ: æˆåŠŸ{sync_result['success_count']}ä¸ª, å¤±è´¥{sync_result['failed_count']}ä¸ª",
                data=sync_result,
                timestamp=datetime.utcnow().isoformat()
            )
            
    except Exception as e:
        logger.error(f"æ•°æ®é›†ESåŒæ­¥å¤±è´¥: {e}")
        return StandardResponse(
            success=False,
            message=f"æ•°æ®é›†ESåŒæ­¥å¤±è´¥: {str(e)}",
            error=str(e),
            timestamp=datetime.utcnow().isoformat()
        ) 