"""
æ•°æ®é›†ç®¡ç†APIè·¯ç”±

æä¾›æ•°æ®é›†çš„ä¸Šä¼ ã€é…ç½®å’ŒæŸ¥è¯¢åŠŸèƒ½
"""

from fastapi import APIRouter, Depends, HTTPException, status, UploadFile, File, Form
from typing import List, Optional, Dict, Any
import os
import uuid
import time
import json
import aiofiles
from pathlib import Path
from datetime import datetime

from ..models import (
    DatasetCreateRequest, DatasetUpdateRequest, DatasetInfo,
    StandardResponse, PaginatedResponse
)
from ..dependencies import get_database, get_settings
from ...utils.schema_database import SchemaDatabase
from ...config.settings import Settings
from ...core.data_analyzer import DataAnalyzer
from ...utils.logger import get_logger, LogContext
from ...utils.geo_converter import GeoConverter  # å¯¼å…¥åœ°ç†æ•°æ®è½¬æ¢å™¨

router = APIRouter(prefix="/datasets", tags=["æ•°æ®é›†ç®¡ç†"])
logger = get_logger(__name__)

def sanitize_filename(filename: str) -> str:
    """
    æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤å±é™©å­—ç¬¦ï¼Œä¿æŒåŸå§‹åç§°çš„å¯è¯»æ€§
    
    Args:
        filename: åŸå§‹æ–‡ä»¶å
        
    Returns:
        str: æ¸…ç†åçš„å®‰å…¨æ–‡ä»¶å
    """
    import re
    
    # ç§»é™¤è·¯å¾„åˆ†éš”ç¬¦å’Œå…¶ä»–å±é™©å­—ç¬¦
    dangerous_chars = r'[<>:"/\\|?*\x00-\x1f]'
    safe_filename = re.sub(dangerous_chars, '_', filename)
    
    # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œç‚¹
    safe_filename = re.sub(r'\.+$', '', safe_filename.strip())
    
    # ç¡®ä¿æ–‡ä»¶åä¸ä¸ºç©º
    if not safe_filename:
        safe_filename = "unnamed_file"
    
    # é™åˆ¶æ–‡ä»¶åé•¿åº¦ï¼ˆWindowsé™åˆ¶ä¸º255å­—ç¬¦ï¼‰
    if len(safe_filename) > 200:
        name_part = safe_filename[:190]
        ext_part = safe_filename[190:]
        safe_filename = name_part + ext_part
    
    return safe_filename

@router.post("/upload", summary="ä¸Šä¼ æ•°æ®æ–‡ä»¶")
async def upload_dataset(
    file: UploadFile = File(..., description="æ•°æ®æ–‡ä»¶(CSV/Excel)"),
    name: str = Form(..., description="æ•°æ®é›†åç§°"),
    description: Optional[str] = Form(None, description="æ•°æ®é›†æè¿°"),
    tree_node_id: Optional[str] = Form(None, description="æ‰€å±èŠ‚ç‚¹ID"),
    auto_analyze: bool = Form(True, description="æ˜¯å¦è‡ªåŠ¨åˆ†æç”Ÿæˆè¯­ä¹‰é…ç½®"),
    db: SchemaDatabase = Depends(get_database)
):
    """
    ä¸Šä¼ æ•°æ®æ–‡ä»¶å¹¶åˆ›å»ºæ•°æ®é›†
    
    æµç¨‹ï¼š
    1. éªŒè¯æ–‡ä»¶æ ¼å¼
    2. éªŒè¯æ ‘èŠ‚ç‚¹ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    3. ä¿å­˜æ–‡ä»¶
    4. å¦‚æœauto_analyzeä¸ºTrueï¼Œè‡ªåŠ¨åˆ†ææ•°æ®ç»“æ„å¹¶ç”Ÿæˆè¯­ä¹‰é…ç½®
    5. åˆ›å»ºæ•°æ®é›†è®°å½•
    """
    
    try:
        with LogContext(logger, f"ä¸Šä¼ æ•°æ®æ–‡ä»¶: {file.filename}"):
            # 1. éªŒè¯æ–‡ä»¶æ ¼å¼
            allowed_extensions = ['.csv', '.shp', '.zip']
            file_ext = Path(file.filename).suffix.lower()
            
            if file_ext not in allowed_extensions:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}ã€‚æ”¯æŒæ ¼å¼: {', '.join(allowed_extensions)}\n\nğŸ’¡ æç¤ºï¼š\n- CSVæ–‡ä»¶ï¼šç›´æ¥ä¸Šä¼ \n- SHPæ–‡ä»¶ï¼šéœ€è¦ä¸Šä¼ åŒ…å«.shpã€.shxã€.dbfç­‰å®Œæ•´æ–‡ä»¶çš„ZIPå‹ç¼©åŒ…\n- å•ç‹¬çš„.shpæ–‡ä»¶æ— æ³•ä½¿ç”¨ï¼Œè¯·æ‰“åŒ…åä¸Šä¼ "
                )
            
            # 2. éªŒè¯æ ‘èŠ‚ç‚¹ï¼ˆå¦‚æœæŒ‡å®šï¼‰
            if tree_node_id and tree_node_id != "0":
                existing_node = db.get_tree_node(tree_node_id)
                if not existing_node:
                    raise HTTPException(
                        status_code=status.HTTP_400_BAD_REQUEST,
                        detail=f"æŒ‡å®šçš„æ ‘èŠ‚ç‚¹ä¸å­˜åœ¨: {tree_node_id}"
                    )
                logger.info(f"æ•°æ®é›†å°†å½’å±åˆ°èŠ‚ç‚¹: {existing_node['name']} (ID: {tree_node_id})")
            
            # 2. ç”Ÿæˆä¸´æ—¶ç›®å½•ï¼ˆå…ˆåˆ›å»ºæ•°æ®é›†è®°å½•è·å–è‡ªå¢IDï¼‰
            temp_id = uuid.uuid4()  # ä¸´æ—¶ç”¨äºç›®å½•åˆ›å»º
            upload_dir = Path("datasets") / str(temp_id)
            upload_dir.mkdir(parents=True, exist_ok=True)
            
            # 3. ä¿å­˜æ–‡ä»¶ï¼ˆä¿ç•™åŸå§‹æ–‡ä»¶åï¼‰
            timestamp = int(time.time())
            safe_filename = sanitize_filename(file.filename)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™æ·»åŠ æ—¶é—´æˆ³é¿å…å†²çª
            file_path = upload_dir / safe_filename
            if file_path.exists():
                name_part = Path(safe_filename).stem
                ext_part = Path(safe_filename).suffix
                safe_filename = f"{name_part}_{timestamp}{ext_part}"
                file_path = upload_dir / safe_filename
            
            logger.info(f"ä¿å­˜æ–‡ä»¶åˆ°: {file_path}")
            
            async with aiofiles.open(file_path, 'wb') as f:
                content = await file.read()
                await f.write(content)
            
            # 3.5. å¤„ç†ZIPæ–‡ä»¶ï¼ˆå¯èƒ½åŒ…å«SHPæ–‡ä»¶ï¼‰
            if file_ext == '.zip':
                logger.info("æ£€æµ‹åˆ°ZIPæ–‡ä»¶ï¼Œå¼€å§‹è§£å‹")
                try:
                    import zipfile
                    
                    # è§£å‹ZIPæ–‡ä»¶ï¼Œå¤„ç†ä¸­æ–‡ç¼–ç é—®é¢˜
                    with zipfile.ZipFile(file_path, 'r') as zip_ref:
                        # æ˜¾ç¤ºZIPæ–‡ä»¶å†…å®¹ç”¨äºè°ƒè¯•
                        file_list = zip_ref.namelist()
                        logger.info(f"ZIPæ–‡ä»¶åŒ…å« {len(file_list)} ä¸ªæ–‡ä»¶:")
                        for file_name in file_list[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªæ–‡ä»¶
                            logger.info(f"  - {file_name}")
                        if len(file_list) > 10:
                            logger.info(f"  ... è¿˜æœ‰ {len(file_list) - 10} ä¸ªæ–‡ä»¶")
                        
                        # å¤„ç†ä¸­æ–‡ç¼–ç é—®é¢˜
                        for member in zip_ref.infolist():
                            # å°è¯•ç”¨GBKç¼–ç è§£ç æ–‡ä»¶åï¼ˆå¤„ç†ä¸­æ–‡ä¹±ç ï¼‰
                            try:
                                # å…ˆå°è¯•UTF-8è§£ç 
                                member.filename = member.filename.encode('cp437').decode('utf-8')
                            except (UnicodeDecodeError, UnicodeEncodeError):
                                try:
                                    # å¦‚æœUTF-8å¤±è´¥ï¼Œå°è¯•GBKè§£ç 
                                    member.filename = member.filename.encode('cp437').decode('gbk')
                                    logger.debug(f"ä½¿ç”¨GBKç¼–ç è§£ç æ–‡ä»¶å: {member.filename}")
                                except (UnicodeDecodeError, UnicodeEncodeError):
                                    # å¦‚æœéƒ½å¤±è´¥ï¼Œä¿æŒåŸå§‹æ–‡ä»¶å
                                    logger.warning(f"æ— æ³•è§£ç æ–‡ä»¶åï¼Œä¿æŒåŸå§‹: {member.filename}")
                            
                            # è§£å‹å•ä¸ªæ–‡ä»¶
                            zip_ref.extract(member, upload_dir)
                    
                    # é€’å½’æŸ¥æ‰¾SHPæ–‡ä»¶ï¼ˆæ”¯æŒå­ç›®å½•ï¼‰
                    shp_files = list(upload_dir.rglob("*.shp"))
                    logger.info(f"åœ¨è§£å‹ç›®å½•ä¸­æ‰¾åˆ° {len(shp_files)} ä¸ªSHPæ–‡ä»¶")
                    
                    if not shp_files:
                        # å°è¯•æŸ¥æ‰¾ä¸åŒå¤§å°å†™çš„æ‰©å±•å
                        shp_files_upper = list(upload_dir.rglob("*.SHP"))
                        if shp_files_upper:
                            shp_files = shp_files_upper
                            logger.info(f"æ‰¾åˆ°å¤§å†™æ‰©å±•åçš„SHPæ–‡ä»¶: {len(shp_files)} ä¸ª")
                        else:
                            # åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶ç”¨äºè°ƒè¯•
                            all_files = list(upload_dir.rglob("*"))
                            logger.error(f"æœªæ‰¾åˆ°SHPæ–‡ä»¶ï¼Œè§£å‹åçš„æ‰€æœ‰æ–‡ä»¶:")
                            for f in all_files:
                                if f.is_file():
                                    logger.error(f"  - {f.relative_to(upload_dir)} (æ‰©å±•å: {f.suffix})")
                            
                            raise HTTPException(
                                status_code=status.HTTP_400_BAD_REQUEST,
                                detail="ZIPæ–‡ä»¶ä¸­æœªæ‰¾åˆ°SHPæ–‡ä»¶\n\nğŸ’¡ è¯·ç¡®ä¿ï¼š\n1. ZIPæ–‡ä»¶ä¸­åŒ…å«.shpæ–‡ä»¶\n2. æ–‡ä»¶æ‰©å±•åæ­£ç¡®(.shpæˆ–.SHP)\n3. æ–‡ä»¶æ²¡æœ‰æŸå"
                            )
                    
                    if len(shp_files) > 1:
                        logger.warning(f"ZIPæ–‡ä»¶ä¸­åŒ…å«å¤šä¸ªSHPæ–‡ä»¶: {[f.name for f in shp_files]}")
                        # é€‰æ‹©ç¬¬ä¸€ä¸ªSHPæ–‡ä»¶ï¼Œä½†ç»™å‡ºè­¦å‘Š
                        shp_file_path = shp_files[0]
                        logger.info(f"é€‰æ‹©ç¬¬ä¸€ä¸ªSHPæ–‡ä»¶: {shp_file_path.name}")
                    else:
                        shp_file_path = shp_files[0]
                    
                    logger.info(f"é€‰ä¸­çš„SHPæ–‡ä»¶: {shp_file_path}")
                    
                    # éªŒè¯å¿…è¦çš„é™„å±æ–‡ä»¶ï¼ˆåœ¨åŒä¸€ç›®å½•ä¸­æŸ¥æ‰¾ï¼‰
                    shp_dir = shp_file_path.parent
                    base_name = shp_file_path.stem
                    required_files = ['.shx', '.dbf']
                    missing_files = []
                    found_files = []
                    
                    for ext in required_files:
                        # å°è¯•ä¸åŒå¤§å°å†™
                        required_file_lower = shp_dir / f"{base_name}{ext}"
                        required_file_upper = shp_dir / f"{base_name}{ext.upper()}"
                        
                        if required_file_lower.exists():
                            found_files.append(ext)
                        elif required_file_upper.exists():
                            found_files.append(ext.upper())
                        else:
                            missing_files.append(ext)
                    
                    logger.info(f"æ‰¾åˆ°çš„é™„å±æ–‡ä»¶: {found_files}")
                    
                    if missing_files:
                        # åˆ—å‡ºSHPæ–‡ä»¶åŒç›®å½•çš„æ‰€æœ‰æ–‡ä»¶
                        same_dir_files = list(shp_dir.glob(f"{base_name}.*"))
                        logger.error(f"SHPæ–‡ä»¶ '{base_name}' åŒç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶:")
                        for f in same_dir_files:
                            logger.error(f"  - {f.name}")
                        
                        raise HTTPException(
                            status_code=status.HTTP_400_BAD_REQUEST,
                            detail=f"ZIPæ–‡ä»¶ä¸­ç¼ºå°‘å¿…è¦çš„Shapefileé™„å±æ–‡ä»¶: {', '.join(missing_files)}\n\nğŸ’¡ å®Œæ•´çš„Shapefileéœ€è¦åŒ…å«:\n- {base_name}.shp (ä¸»æ–‡ä»¶)\n- {base_name}.shx (ç´¢å¼•æ–‡ä»¶)\n- {base_name}.dbf (å±æ€§æ–‡ä»¶)\n\nå½“å‰åªæ‰¾åˆ°: {', '.join(found_files)}"
                        )
                    
                    # æ›´æ–°file_pathä¸ºSHPæ–‡ä»¶è·¯å¾„ï¼Œå¹¶æ ‡è®°ä¸ºä»ZIPè§£å‹
                    file_path = shp_file_path
                    file_ext = '.shp'  # åç»­æŒ‰SHPæ–‡ä»¶å¤„ç†
                    logger.info(f"ZIPæ–‡ä»¶è§£å‹æˆåŠŸï¼Œå°†æŒ‰SHPæ–‡ä»¶å¤„ç†: {file_path}")
                    
                except zipfile.BadZipFile:
                    raise HTTPException(
                        status_code=status.HTTP_400_BAD_REQUEST,
                        detail="æ— æ•ˆçš„ZIPæ–‡ä»¶æ ¼å¼"
                    )
                except Exception as e:
                    logger.error(f"ZIPæ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}")
                    raise HTTPException(
                        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                        detail=f"ZIPæ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}"
                    )

            # 3.6. å¤„ç†SHPæ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
            converted_csv_path = None
            actual_data_path = str(file_path)  # ç”¨äºåç»­åˆ†æçš„å®é™…æ•°æ®æ–‡ä»¶è·¯å¾„
            conversion_info = {}  # åˆå§‹åŒ–è½¬æ¢ä¿¡æ¯
            
            if file_ext == '.shp':
                logger.info("æ£€æµ‹åˆ°SHPæ–‡ä»¶ï¼Œå¼€å§‹è½¬æ¢ä¸ºCSV")
                try:
                    # éªŒè¯SHPæ–‡ä»¶
                    if not GeoConverter.validate_shp_file(str(file_path)):
                        raise HTTPException(
                            status_code=status.HTTP_400_BAD_REQUEST,
                            detail="SHPæ–‡ä»¶æ— æ•ˆæˆ–ç¼ºå°‘å¿…è¦çš„é™„å±æ–‡ä»¶ï¼ˆ.shx, .dbfï¼‰"
                        )
                    
                    # è·å–SHPæ–‡ä»¶ä¿¡æ¯
                    shp_info = GeoConverter.get_shp_info(str(file_path))
                    logger.info(f"SHPæ–‡ä»¶ä¿¡æ¯: {shp_info}")
                    
                    # è½¬æ¢ä¸ºCSVï¼ˆåŸºäºåŸå§‹æ–‡ä»¶åï¼‰
                    csv_filename = f"{Path(safe_filename).stem}.csv"
                    converted_csv_path = upload_dir / csv_filename
                    
                    # è½¬æ¢SHPåˆ°CSV
                    GeoConverter.shp_to_csv(str(file_path), str(converted_csv_path))
                    logger.info(f"SHPæ–‡ä»¶å·²è½¬æ¢ä¸ºCSV: {converted_csv_path}")
                    
                    # æ›´æ–°å®é™…æ•°æ®æ–‡ä»¶è·¯å¾„
                    actual_data_path = str(converted_csv_path)
                    
                    # è®¾ç½®è½¬æ¢ä¿¡æ¯
                    conversion_info = {
                        "shp_info": shp_info,
                        "to_format": "csv",
                        "from_format": "shp",
                        "original_file": str(file_path),
                        "converted_file": str(converted_csv_path),
                        "conversion_time": datetime.utcnow().timestamp()
                    }
                    
                except Exception as e:
                    logger.error(f"SHPæ–‡ä»¶è½¬æ¢å¤±è´¥: {str(e)}")
                    raise HTTPException(
                        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                        detail=f"SHPæ–‡ä»¶è½¬æ¢å¤±è´¥: {str(e)}"
                    )
            
            # 4. å…ˆè·å–æ•°æ®é›†IDï¼ˆä½¿ç”¨ä¸´æ—¶æ•°æ®åˆ›å»ºï¼‰
            temp_dataset_data = {
                'name': name,
                'description': description,
                'file_path': 'temp',  # ä¸´æ—¶è·¯å¾„ï¼Œç¨åæ›´æ–°
                'original_filename': file.filename,
                'original_file_type': file_ext.lstrip('.'),
                'actual_data_path': 'temp',  # ä¸´æ—¶è·¯å¾„ï¼Œç¨åæ›´æ–°
                'data_file_type': 'csv',
                'is_converted': file_ext != '.csv',
                'conversion_info': json.dumps(conversion_info) if conversion_info else json.dumps({}),
                'tree_node_id': tree_node_id,  # æ·»åŠ æ ‘èŠ‚ç‚¹ID
                'status': 'active',
                'created_at': datetime.utcnow().timestamp(),
                'updated_at': datetime.utcnow().timestamp()
            }
            
            # ä½¿ç”¨ç°æœ‰çš„save_datasetæ–¹æ³•è·å–è‡ªå¢ID
            dataset_id = db.save_dataset(temp_dataset_data)
            logger.info(f"è·å–åˆ°æ•°æ®é›†ID: {dataset_id}")
            
            # 5. é‡å‘½åç›®å½•ä¸ºå®é™…IDï¼Œå¹¶æ›´æ–°æ‰€æœ‰è·¯å¾„
            final_upload_dir = Path("datasets") / str(dataset_id)
            final_file_path = None
            final_actual_data_path = None
            
            try:
                # åœ¨Windowsä¸Šï¼Œå…ˆç¡®ä¿æ²¡æœ‰æ–‡ä»¶å¥æŸ„å ç”¨
                import shutil
                
                # å¦‚æœç›®æ ‡ç›®å½•å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
                if final_upload_dir.exists():
                    shutil.rmtree(final_upload_dir)
                
                # ä½¿ç”¨shutil.moveä»£æ›¿renameï¼Œæ›´å¯é 
                shutil.move(str(upload_dir), str(final_upload_dir))
                logger.info(f"é‡å‘½åç›®å½•æˆåŠŸ: {upload_dir} -> {final_upload_dir}")
                
                # è®¡ç®—æœ€ç»ˆè·¯å¾„ï¼ˆä½¿ç”¨å­—ç¬¦ä¸²æ›¿æ¢ä»£æ›¿relative_toï¼Œæ›´ç¨³å®šï¼‰
                old_dir_str = str(upload_dir).replace('\\', '/')
                new_dir_str = str(final_upload_dir).replace('\\', '/')
                
                final_file_path = str(file_path).replace('\\', '/').replace(old_dir_str, new_dir_str)
                final_actual_data_path = str(actual_data_path).replace('\\', '/').replace(old_dir_str, new_dir_str)
                
                logger.info(f"æœ€ç»ˆæ–‡ä»¶è·¯å¾„: {final_file_path}")
                logger.info(f"æœ€ç»ˆæ•°æ®è·¯å¾„: {final_actual_data_path}")
                
            except Exception as e:
                logger.warning(f"é‡å‘½åç›®å½•å¤±è´¥ï¼Œä¿æŒåŸç›®å½•: {e}")
                # å¦‚æœé‡å‘½åå¤±è´¥ï¼Œä½¿ç”¨åŸè·¯å¾„
                final_file_path = str(file_path)
                final_actual_data_path = str(actual_data_path)
            
            # 6. æ›´æ–°æ•°æ®åº“ä¸­çš„æ­£ç¡®è·¯å¾„
            try:
                db.update_dataset(dataset_id, {
                    'file_path': final_file_path,
                    'actual_data_path': final_actual_data_path
                })
                logger.info(f"æ•°æ®åº“è·¯å¾„æ›´æ–°æˆåŠŸ")
                logger.info(f"æ›´æ–°çš„file_path: {final_file_path}")
                logger.info(f"æ›´æ–°çš„actual_data_path: {final_actual_data_path}")
            except Exception as update_e:
                logger.error(f"æ›´æ–°æ•°æ®åº“è·¯å¾„å¤±è´¥: {update_e}")
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail=f"æ›´æ–°æ•°æ®åº“è·¯å¾„å¤±è´¥: {update_e}"
                )
            
            # 7. å¦‚æœå¯ç”¨äº†è‡ªåŠ¨åˆ†æï¼Œç”Ÿæˆè¯­ä¹‰é…ç½®
            if auto_analyze:
                try:
                    # è¯»å–æ•°æ®ç”¨äºåˆ†æï¼ˆä½¿ç”¨æœ€ç»ˆçš„æ•°æ®æ–‡ä»¶è·¯å¾„ï¼‰
                    df = DataAnalyzer.load_data(final_actual_data_path)
                    
                    # åˆ›å»ºåŸºç¡€è¯­ä¹‰é…ç½®
                    schema = DataAnalyzer.create_semantic_schema(
                        file_path=final_actual_data_path,
                        organization="ask-data-ai",
                        dataset_name=name.lower().replace(' ', '-')
                    )
                    
                    # è‡ªåŠ¨ç”Ÿæˆtransformationsé…ç½®
                    transformations = DataAnalyzer.auto_generate_transformations(df)
                    if transformations:
                        schema['transformations'] = transformations
                        logger.info(f"è‡ªåŠ¨ç”Ÿæˆäº† {len(transformations)} ä¸ªtransformationé…ç½®")
                    
                    # ä¿å­˜è¯­ä¹‰é…ç½®
                    db.save_schema(dataset_id, schema)
                    logger.info("è‡ªåŠ¨ç”Ÿæˆè¯­ä¹‰é…ç½®æˆåŠŸ")
                    
                except Exception as e:
                    logger.error(f"è‡ªåŠ¨ç”Ÿæˆè¯­ä¹‰é…ç½®å¤±è´¥: {str(e)}")
                    # ä¸å½±å“æ•°æ®é›†åˆ›å»ºï¼Œåªè®°å½•é”™è¯¯
            
            return StandardResponse(
                success=True,
                message="æ•°æ®é›†åˆ›å»ºæˆåŠŸ",
                data={
                    'id': dataset_id,
                    'name': name,
                    'description': description,
                    'file_path': final_file_path,
                    'actual_data_path': final_actual_data_path,
                    'original_filename': file.filename,
                    'tree_node_id': tree_node_id,
                    'is_converted': file_ext != '.csv',
                    'status': 'active',
                    'created_at': datetime.utcnow().isoformat()
                },
                timestamp=datetime.utcnow().isoformat()
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ•°æ®é›†åˆ›å»ºå¤±è´¥: {str(e)}")
        return StandardResponse(
            success=False,
            message="æ•°æ®é›†åˆ›å»ºå¤±è´¥",
            error=str(e),
            timestamp=datetime.utcnow().isoformat()
        )

@router.get("/", response_model=PaginatedResponse, summary="è·å–æ•°æ®é›†åˆ—è¡¨")
async def list_datasets(
    page: int = 1,
    per_page: int = 20,
    tree_node_id: Optional[str] = None,
    db: SchemaDatabase = Depends(get_database)
):
    """
    è·å–æ•°æ®é›†åˆ—è¡¨
    
    Args:
        page: é¡µç ï¼Œä»1å¼€å§‹
        per_page: æ¯é¡µæ•°é‡
        tree_node_id: æ ‘èŠ‚ç‚¹IDï¼Œå¦‚æœæŒ‡å®šåˆ™è¿”å›è¯¥èŠ‚ç‚¹åŠå…¶å­èŠ‚ç‚¹ä¸‹çš„æ•°æ®é›†
                     å¦‚æœä¸ä¼ æˆ–ä¼ "0"ï¼Œåˆ™è¿”å›æ‰€æœ‰æ•°æ®é›†
    """
    try:
        # è°ƒç”¨å¸¦tree_node_idå‚æ•°çš„list_datasetsæ–¹æ³•
        all_datasets = db.list_datasets(tree_node_id=tree_node_id)
        
        # æ‰‹åŠ¨å®ç°åˆ†é¡µ
        total = len(all_datasets)
        start_idx = (page - 1) * per_page
        end_idx = start_idx + per_page
        datasets = all_datasets[start_idx:end_idx]
        
        return PaginatedResponse(
            success=True,
            message=f"è·å–æ•°æ®é›†åˆ—è¡¨æˆåŠŸ (tree_node_id: {tree_node_id or 'all'})",
            data=datasets,
            pagination={
                "total": total,
                "page": page,
                "per_page": per_page,
                "pages": (total + per_page - 1) // per_page
            },
            timestamp=datetime.utcnow().isoformat()
        )
    except Exception as e:
        logger.error(f"è·å–æ•°æ®é›†åˆ—è¡¨å¤±è´¥: {str(e)}")
        # é”™è¯¯æ—¶ä¹Ÿè¦è¿”å›PaginatedResponseæ ¼å¼
        return PaginatedResponse(
            success=False,
            message="è·å–æ•°æ®é›†åˆ—è¡¨å¤±è´¥",
            data=[],
            pagination={
                "total": 0,
                "page": page,
                "per_page": per_page,
                "pages": 0
            },
            error=str(e),
            timestamp=datetime.utcnow().isoformat()
        )

@router.get("/{dataset_id}", response_model=StandardResponse, summary="è·å–æ•°æ®é›†è¯¦ç»†ä¿¡æ¯")
async def get_dataset(
    dataset_id: int,
    db: SchemaDatabase = Depends(get_database)
):
    """è·å–æ•°æ®é›†çš„è¯¦ç»†ä¿¡æ¯"""
    try:
        dataset = db.get_dataset_by_id(dataset_id)
        if not dataset:
            return StandardResponse(
                success=False,
                message="æ•°æ®é›†ä¸å­˜åœ¨",
                error=f"æ•°æ®é›† {dataset_id} ä¸å­˜åœ¨",
                timestamp=datetime.utcnow().isoformat()
            )
        
        return StandardResponse(
            success=True,
            message="è·å–æ•°æ®é›†æˆåŠŸ",
            data=dataset,
            timestamp=datetime.utcnow().isoformat()
        )
    except Exception as e:
        logger.error(f"è·å–æ•°æ®é›†å¤±è´¥: {str(e)}")
        return StandardResponse(
            success=False,
            message="è·å–æ•°æ®é›†å¤±è´¥",
            error=str(e),
            timestamp=datetime.utcnow().isoformat()
        )

@router.put("/{dataset_id}", response_model=StandardResponse, summary="æ›´æ–°æ•°æ®é›†ä¿¡æ¯")
async def update_dataset(
    dataset_id: int,
    dataset: DatasetUpdateRequest,
    db: SchemaDatabase = Depends(get_database)
):
    """æ›´æ–°æ•°æ®é›†çš„ä¿¡æ¯"""
    try:
        # éªŒè¯æ•°æ®é›†æ˜¯å¦å­˜åœ¨
        existing_dataset = db.get_dataset_by_id(dataset_id)
        if not existing_dataset:
            return StandardResponse(
                success=False,
                message="æ•°æ®é›†ä¸å­˜åœ¨",
                error=f"æ•°æ®é›† {dataset_id} ä¸å­˜åœ¨",
                timestamp=datetime.utcnow().isoformat()
            )
        
        # æ„å»ºæ›´æ–°æ•°æ®
        update_data = {}
        if dataset.name is not None:
            update_data['name'] = dataset.name
        if dataset.description is not None:
            update_data['description'] = dataset.description
        if dataset.file_path is not None:
            update_data['file_path'] = dataset.file_path
        if dataset.status is not None:
            update_data['status'] = dataset.status
        
        # ä½¿ç”¨ç°æœ‰çš„update_datasetæ–¹æ³•
        success = db.update_dataset(dataset_id, update_data)
        if success:
            # è·å–æ›´æ–°åçš„æ•°æ®é›†ä¿¡æ¯
            updated_dataset = db.get_dataset_by_id(dataset_id)
            return StandardResponse(
                success=True,
                message="æ•°æ®é›†æ›´æ–°æˆåŠŸ",
                data=updated_dataset,
                timestamp=datetime.utcnow().isoformat()
            )
        else:
            return StandardResponse(
                success=False,
                message="æ•°æ®é›†æ›´æ–°å¤±è´¥",
                error="æ›´æ–°æ“ä½œå¤±è´¥",
                timestamp=datetime.utcnow().isoformat()
            )
    except Exception as e:
        logger.error(f"æ›´æ–°æ•°æ®é›†å¤±è´¥: {str(e)}")
        return StandardResponse(
            success=False,
            message="æ›´æ–°æ•°æ®é›†å¤±è´¥",
            error=str(e),
            timestamp=datetime.utcnow().isoformat()
        )

@router.delete("/{dataset_id}", response_model=StandardResponse, summary="åˆ é™¤æ•°æ®é›†")
async def delete_dataset(
    dataset_id: int,
    delete_file: bool = True,
    db: SchemaDatabase = Depends(get_database)
):
    """åˆ é™¤æ•°æ®é›†"""
    try:
        # éªŒè¯æ•°æ®é›†æ˜¯å¦å­˜åœ¨
        dataset = db.get_dataset_by_id(dataset_id)
        if not dataset:
            return StandardResponse(
                success=False,
                message="æ•°æ®é›†ä¸å­˜åœ¨",
                error=f"æ•°æ®é›† {dataset_id} ä¸å­˜åœ¨",
                timestamp=datetime.utcnow().isoformat()
            )
        
        # ä½¿ç”¨ç°æœ‰çš„delete_dataset_by_idæ–¹æ³•
        success = db.delete_dataset_by_id(dataset_id)
        if success:
            return StandardResponse(
                success=True,
                message="æ•°æ®é›†åˆ é™¤æˆåŠŸ",
                timestamp=datetime.utcnow().isoformat()
            )
        else:
            return StandardResponse(
                success=False,
                message="æ•°æ®é›†åˆ é™¤å¤±è´¥",
                error="åˆ é™¤æ“ä½œå¤±è´¥",
                timestamp=datetime.utcnow().isoformat()
            )
    except Exception as e:
        logger.error(f"åˆ é™¤æ•°æ®é›†å¤±è´¥: {str(e)}")
        return StandardResponse(
            success=False,
            message="åˆ é™¤æ•°æ®é›†å¤±è´¥",
            error=str(e),
            timestamp=datetime.utcnow().isoformat()
        ) 